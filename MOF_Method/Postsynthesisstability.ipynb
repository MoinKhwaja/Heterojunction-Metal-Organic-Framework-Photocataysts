{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import flask\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import stat\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import sklearn\n",
    "import smtplib\n",
    "import glob\n",
    "import uuid\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.resources import CDN\n",
    "from bokeh.embed import file_html\n",
    "from bokeh.palettes import Inferno256\n",
    "from flask import request, session\n",
    "# import flask_login\n",
    "# from flask_login import LoginManager, UserMixin, login_required, current_user\n",
    "from molSimplify.Informatics.MOF.MOF_descriptors import get_primitive, get_MOF_descriptors\n",
    "from flask_cors import CORS\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from werkzeug.utils import secure_filename\n",
    "from list_content.list_content import my_linkers, my_sbus, my_nets, my_MOFs\n",
    "\n",
    "\n",
    "cmap_bokeh = Inferno256\n",
    "\n",
    "MOFSIMPLIFY_PATH = os.path.abspath('.') # the main directory\n",
    "MOFSIMPLIFY_PATH += '/'\n",
    "USE_SPLASH_PAGE = False\n",
    "\n",
    "app = flask.Flask(__name__)\n",
    "app.config['MAX_CONTENT_LENGTH'] = 1024 * 1024 * 20 # Upload max 20 megabytes\n",
    "app.config['UPLOAD_EXTENSIONS'] = ['.jpg', '.jpeg', '.png', '.pdf', '.tiff', '.tif', '.eps'] # acceptable file types\n",
    "    # Note: these are a superset of the extensions indicated on the form and allowed on the front end, so some of these extensions don't actually apply\n",
    "app.secret_key = str(json.load(open('secret_key.json','r'))['key']) # secret key\n",
    "cors = CORS(app)\n",
    "\n",
    "operation_counter = 0 # This variable keeps track of server traffic to alert the user if they should wait until later to make a request. \n",
    "    # I only change operation_counter for the more time-consuming operations (the two predictions, and component analysis).\n",
    "    # operation_counter is periodically set back to zero, since if a user closes their browser in the middle of an operation operation_counter is not properly -=1'd\n",
    "last_operation_counter_clear = time.time() # the current time when server is started\n",
    "\n",
    "MAX_OPERATIONS = 4 # This variable dictates the maximum number of concurrent operations, to prevent server overload.\n",
    "\n",
    "# The following three functions are needed for the ANN models.\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "# global variables for the ANN models, loaded once when server first started\n",
    "solvent_ANN_path = MOFSIMPLIFY_PATH + 'model/solvent/ANN/' \n",
    "thermal_ANN_path = MOFSIMPLIFY_PATH + 'model/thermal/ANN/'\n",
    "dependencies = {'precision':precision,'recall':recall,'f1':f1}\n",
    "# loading the ANN models to save time later\n",
    "tf_session = tf.Session()\n",
    "from tensorflow import keras as tf_keras\n",
    "tf_keras.backend.set_session(tf_session)\n",
    "##### Note: the h5 models for the solvent removal stability prediction ANN and the thermal stability prediction ANN should be based on the same version of TensorFlow (here, 1.14). #####\n",
    "solvent_model = keras.models.load_model(solvent_ANN_path + 'final_model_flag_few_epochs.h5',custom_objects=dependencies)\n",
    "thermal_model = keras.models.load_model(thermal_ANN_path + 'final_model_T_few_epochs.h5',custom_objects=dependencies)\n",
    "\n",
    "\n",
    "# global variable dictionary for stable building block aliases. See https://zenodo.org/record/7091192\n",
    "bb_mapping = {'XUDNAN_clean_linker_2': 'E0', 'MIFKUJ_clean_linker_0': 'E1', 'AJUNOK_clean_linker_0': 'E2', 'OJICUG_clean_linker_0': 'E3',\n",
    " 'FUNLAD_clean_linker_1': 'E4', 'CIFDUS_clean_linker_0': 'E5', 'EYOCIG_clean_linker_4': 'E6', 'TEMPAE_clean_linker_10': 'E7',\n",
    " 'KIFKEQ_clean_linker_1': 'E8', 'FUNLAD_clean_linker_0': 'E9', 'EZIPEK_clean_linker_0': 'E10', 'UKIBIB_clean_linker_0': 'E11',\n",
    " 'UKIBUN_clean_linker_0': 'E12', 'UVAHIK_clean_linker_0': 'E13', 'XIVTED_clean_linker_0': 'E14', 'ZUJKAS_clean_linker_0': 'E15',\n",
    " 'APAZEY_clean_linker_0': 'orgN0', 'AZAVOO_clean_linker_0': 'orgN1', 'BELTOD_clean_linker_0': 'orgN2', 'BETDAH_clean_linker_0': 'orgN3',\n",
    " 'BETDEL_clean_linker_0': 'orgN4', 'BETDIP_clean_linker_0': 'orgN5', 'CEKHIL_clean_linker_0': 'orgN6', 'CUKXOW_clean_linker_0': 'orgN7',\n",
    " 'CUQRIR_clean_linker_1': 'orgN8', 'ESEHIV_clean_linker_0': 'orgN9', 'GIZVER_clean_linker_0': 'orgN10', 'INOVEN_clean_linker_0': 'orgN11',\n",
    " 'IYUCEM_clean_linker_1': 'orgN12', 'IZENUY_clean_linker_0': 'orgN13', 'KAKCAD_clean_linker_0': 'orgN14', 'KUFVIS_clean_linker_0': 'orgN15',\n",
    " 'MAKGUD_clean_linker_0': 'orgN16', 'MIDCAF_clean_linker_0': 'orgN17', 'MIFKUJ_clean_linker_1': 'orgN18', 'MUDLON_clean_linker_0': 'orgN19',\n",
    " 'NAHDIM_clean_linker_0': 'orgN20', 'NAWXER_clean_linker_0': 'orgN21', 'PAMTOO_clean_linker_0': 'orgN22', 'QEWDON_clean_linker_0': 'orgN23',\n",
    " 'QUQFOY_clean_linker_2': 'orgN24', 'TATPOV_clean_linker_0': 'orgN25', 'VETTIZ_charged_linker_0': 'orgN26', 'WOSHET_clean_linker_1': 'orgN27',\n",
    " 'BEQFEK_clean_linker_11': 'orgN28', 'LEJCEK_clean_linker_0': 'orgN29', 'UFIREI_clean_linker_0': 'orgN30', 'UWAGAB01_clean_linker_0': 'orgN31',\n",
    " 'BEQFEK_clean_sbu_0': 'N0', 'BOTCEU_clean_sbu_0': 'N1', 'ENOWUB_clean_sbu_0': 'N2', 'FANWIC_clean_sbu_0': 'N3',\n",
    " 'ICAMEG_clean_sbu_1': 'N4', 'LIZSOE_clean_sbu_0': 'N5', 'UKALOJ_clean_sbu_0': 'N6', 'UKIBUN_clean_sbu_0': 'N7',\n",
    " 'ZALLEG_clean_sbu_0': 'N8', 'AJUNOK_clean_sbu_0': 'N9', 'AZAVOO_clean_sbu_0': 'N10', 'BELTOD_clean_sbu_0': 'N11',\n",
    " 'BETDAH_clean_sbu_0': 'N12', 'BETDEL_clean_sbu_0': 'N13', 'BETDIP_clean_sbu_0': 'N14', 'BETFAJ_clean_sbu_0': 'N15',\n",
    " 'BETFEN_clean_sbu_0': 'N16', 'BETGAK_clean_sbu_0': 'N17', 'CEKHIL_clean_sbu_0': 'N18', 'CIFDUS_clean_sbu_1': 'N19',\n",
    " 'CUKXOW_clean_sbu_0': 'N20', 'CUWYAW_clean_sbu_0': 'N21', 'EBIMEJ_clean_sbu_0': 'N22', 'EQERAU_clean_sbu_0': 'N23',\n",
    " 'ESEHIV_clean_sbu_0': 'N24', 'EYACOX_clean_sbu_0': 'N25', 'EYACOX_clean_sbu_1': 'N26', 'EZIPEK_clean_sbu_0': 'N27',\n",
    " 'FUNLAD_clean_sbu_0': 'N28', 'GALJAG_clean_sbu_0': 'N29', 'GEDQOX_clean_sbu_0': 'N30', 'GIZVER_clean_sbu_0': 'N31',\n",
    " 'HICVOG_clean_sbu_0': 'N32', 'HISSIN_clean_sbu_0': 'N33', 'HISSIN_clean_sbu_1': 'N34', 'ICIZOL_clean_sbu_0': 'N35',\n",
    " 'INOVEN_clean_sbu_0': 'N36', 'IYUCEM_clean_sbu_0': 'N37', 'IZENUY_clean_sbu_0': 'N38', 'JUFBIX_clean_sbu_0': 'N39',\n",
    " 'KAKCAD_clean_sbu_0': 'N40', 'KOZSID_clean_sbu_0': 'N41', 'KUFVIS_clean_sbu_0': 'N42', 'KUMBOL_clean_sbu_0': 'N43',\n",
    " 'KUMBOL_clean_sbu_1': 'N44', 'KUMBUR_clean_sbu_0': 'N45', 'KUMBUR_clean_sbu_1': 'N46', 'KUMJIN_clean_sbu_0': 'N47',\n",
    " 'KUMJIN_clean_sbu_1': 'N48', 'LEVNOQ01_clean_sbu_1': 'N49', 'MAKGOX_clean_sbu_0': 'N50', 'MAKGUD_clean_sbu_1': 'N51',\n",
    " 'MIDCAF_clean_sbu_0': 'N52', 'MIFKUJ_clean_sbu_0': 'N53', 'MUDLON_clean_sbu_0': 'N54', 'NAHDIM_clean_sbu_0': 'N55',\n",
    " 'NAHDIM_clean_sbu_2': 'N56', 'NAWXER_clean_sbu_0': 'N57', 'NUHQIS_clean_sbu_0': 'N58', 'NUHQUE_clean_sbu_2': 'N59',\n",
    " 'NUHRAL_clean_sbu_3': 'N60', 'OJICUG_clean_sbu_0': 'N61', 'OLANAS_clean_sbu_1': 'N62', 'OLANAS_clean_sbu_2': 'N63',\n",
    " 'OLANEW_clean_sbu_0': 'N64', 'OLANEW_clean_sbu_1': 'N65', 'OLANEW_clean_sbu_2': 'N66', 'OLANEW_clean_sbu_3': 'N67',\n",
    " 'OLANEW_clean_sbu_4': 'N68', 'PAMTOO_clean_sbu_0': 'N69', 'PAMTUU_clean_sbu_0': 'N70', 'PORLAL_clean_sbu_0': 'N71',\n",
    " 'QEWDON_clean_sbu_0': 'N72', 'QUQFOY_clean_sbu_1': 'N73', 'QUQFOY_clean_sbu_3': 'N74', 'SARMOO_clean_sbu_0': 'N75',\n",
    " 'TAGTUT_clean_sbu_1': 'N76', 'TAGTUT_clean_sbu_3': 'N77', 'TATPOV_clean_sbu_0': 'N78', 'UXUYUI_clean_sbu_0': 'N79',\n",
    " 'VETTIZ_charged_sbu_0': 'N80', 'VOLQOD_clean_sbu_1': 'N81', 'WAQDOJ_charged_sbu_0': 'N82', 'WUSLED_clean_sbu_0': 'N83',\n",
    " 'XADDAJ01_clean_sbu_3': 'N84', 'XOMCOT_clean_sbu_0': 'N85', 'XOMCOT_clean_sbu_1': 'N86', 'XUDNAN_clean_sbu_1': 'N87'}\n",
    "\n",
    "# Loading the names of MOFs from our hypothetical database. Constructed with ultrastable building blocks.\n",
    "with open('pickle_files/MOF_names/1inorganic_1edge_cifs_list.pkl', 'rb') as f:\n",
    "    list_1inorganic_1edge_MOFs = pkl.load(f)\n",
    "with open('pickle_files/MOF_names/1inorganic_1organic_1edge_cifs_list.pkl', 'rb') as f:\n",
    "    list_1inorganic_1organic_1edge_MOFs = pkl.load(f)\n",
    "with open('pickle_files/MOF_names/2inorganic_1edge_cifs_list.pkl', 'rb') as f:\n",
    "    list_2inorganic_1edge_MOFs = pkl.load(f)\n",
    "\n",
    "# Loading the names of the ultrastable MOFs from our hypothetical database.\n",
    "with open('pickle_files/ultrastable_MOFs/1inor_1edge_cifs_list_ultrastable.pkl', 'rb') as f:\n",
    "    list_1inorganic_1edge_MOFs_ultrastable = pkl.load(f)\n",
    "with open('pickle_files/ultrastable_MOFs/1inor_1org_1edge_cifs_list_ultrastable.pkl', 'rb') as f:\n",
    "    list_1inorganic_1organic_1edge_MOFs_ultrastable = pkl.load(f)\n",
    "with open('pickle_files/ultrastable_MOFs/2inor_1edge_cifs_list_ultrastable.pkl', 'rb') as f:\n",
    "    list_2inorganic_1edge_MOFs_ultrastable = pkl.load(f)\n",
    "\n",
    "ultrastable_MOFs = list_1inorganic_1edge_MOFs_ultrastable + list_1inorganic_1organic_1edge_MOFs_ultrastable + list_2inorganic_1edge_MOFs_ultrastable\n",
    "\n",
    "def conditional_diminish(counter):\n",
    "    \"\"\"\n",
    "    conditional_diminish decreases the input by one and returns it, unless the input is already zero.\n",
    "    The input is intended to be operation_counter.\n",
    "    operation_counter might be zero when conditional_diminish is called b/c of the periodic zero-ing of operation_counter. It is unlikely though.\n",
    "\n",
    "    :param counter: An int, which should be operation_counter.\n",
    "    :return: The new value of operation_counter.\n",
    "    \"\"\" \n",
    "    if counter != 0:\n",
    "        counter -= 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "# app.route takes jquery ($) requests from index.html and executes the associated function in app.py.\n",
    "# Output can then be returned to index.html.\n",
    "\n",
    "def set_ID():\n",
    "    \"\"\"\n",
    "    set_ID sets the session user ID. \n",
    "    This is also used to generate unique folders, so that multiple users can use the website at a time. \n",
    "        The user's folder is temp_file_creation_[ID]\n",
    "    Specifically, the function copies the temp_file_creation folder for the current user so that the rest of MOFSimplify functionality can be executed for the current user in their own folder.\n",
    "        This lets multiple operations be run for different users concurrently.\n",
    "    This function also deletes temp_file_creation copies from other users that have not been used for a while, in order to reduce clutter.\n",
    "\n",
    "    :return: The session ID for this user.\n",
    "    \"\"\" \n",
    "\n",
    "    session['ID'] = uuid.uuid4() # a unique ID for this session\n",
    "    session['permission'] = True # keeps track of if user gave us permission to store the MOFs they predict on; defaults to Yes\n",
    "\n",
    "    # make a version of the temp_file_creation folder for this user\n",
    "    new_folder = MOFSIMPLIFY_PATH + '/temp_file_creation_' + str(session['ID'])\n",
    "    shutil.copytree('temp_file_creation', new_folder)\n",
    "    os.remove(new_folder + '/temp_cif.cif') # remove this, for sole purpose of updating time stamp on the new folder (copytree doesn't)\n",
    "\n",
    "    # delete all temp_file_creation clone folders that haven't been used for a while, to prevent folder accumulation\n",
    "    for root, dirs, files in os.walk(MOFSIMPLIFY_PATH):\n",
    "        for dir in dirs:\n",
    "            target_str = 'temp_file_creation'\n",
    "            if len(dir) > len(target_str) and target_str in dir and file_age_in_seconds(dir) > 7200: # 7200s is two hours\n",
    "                # target_str in dir since the names of all copies start with the sequence \"temp_file_creation\"\n",
    "                # len(dir) > len(target_str) to prevent deleting the original temp_file_creation folder\n",
    "                shutil.rmtree(dir)\n",
    "\n",
    "    return str(session['ID']) # return a string\n",
    "\n",
    "@app.route('/get_ID', methods=['GET'])\n",
    "def get_ID():\n",
    "    \"\"\"\n",
    "    get_ID gets the session user ID. \n",
    "    This is used for getting building block generated MOFs.\n",
    "\n",
    "    :return: string, The session ID for this user.\n",
    "    \"\"\" \n",
    "    return str(session['ID']) # return a string\n",
    "\n",
    "@app.route('/permission', methods=['POST'])\n",
    "def change_permission():\n",
    "    \"\"\"\n",
    "    change_permission adjusts whether or not MOFSimplify stores information on the MOFs the user predicts on.\n",
    "\n",
    "    If the user clicks \"Yes\" or \"No\" before get_lists() finishes running upon the website's startup in the browser (~4 seconds), their \"Yes\" or \"No\" input will not register for the session and will register for another session (the most recently started one) instead.\n",
    "    TODO perhaps adjust for this down the line. Show the Yes and No options only after the user has a MOF queued up.\n",
    "\n",
    "    :return: string, The boolean sent from the front end. We return this because we have to return something, but nothing is done with the returned value on the front end.\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab data\n",
    "    permission = json.loads(flask.request.get_data())\n",
    "    session['permission'] = permission\n",
    "    print('Permission check')\n",
    "    print(permission)\n",
    "\n",
    "    return str(permission)\n",
    "\n",
    "@app.route('/list_getter', methods=['GET'])\n",
    "def get_lists():\n",
    "    \"\"\"\n",
    "    get_lists gets the dropdown lists. \n",
    "\n",
    "    :return: dictionary. The dropdown lists\n",
    "    \"\"\" \n",
    "\n",
    "    # Initializes the new user, since this function is called when the browser is first opened up.\n",
    "    set_ID()\n",
    "\n",
    "    return {'my_linkers':my_linkers, 'my_sbus':my_sbus, 'my_nets':my_nets, 'my_MOFs':my_MOFs}\n",
    "\n",
    "\n",
    "\n",
    "## Handle feedback\n",
    "@app.route('/process_feedback', methods=['POST'])\n",
    "def process_feedback():\n",
    "    \"\"\"\n",
    "    process_feedback inserts MOFSimplify form feedback into the MongoDB feedback database. \n",
    "    If an uploaded file has an incorrect extension (i.e. is a disallowed file format), the user is directed to an error page.\n",
    "    \"\"\" \n",
    "    client = MongoClient('18.18.63.68',27017) # connect to mongodb\n",
    "        # The first argument is the IP address. The second argument is the port.\n",
    "    db = client.feedback\n",
    "    collection = db.MOFSimplify # The MOFSimplify collection in the feedback database.\n",
    "    fields = ['feedback_form_name', 'rating', 'email', 'reason', 'comments', 'cif_file_name', 'structure', 'solvent']\n",
    "    #$meta_fields = ['IP', 'datetime', 'cif_file', 'MOF_name']\n",
    "    final_dict = {}\n",
    "    for field in fields:\n",
    "        final_dict[field] = request.form.get(field)\n",
    "\n",
    "    # Populate special fields\n",
    "    uploaded_file = request.files['file']\n",
    "    if uploaded_file.filename == '' and request.form.get('feedback_form_name') != 'upload_form':\n",
    "        # User did not upload the optional TGA trace\n",
    "        print('No TGA trace')\n",
    "    # if final_dict['file']==b'':\n",
    "    #     file_ext = ''\n",
    "    else:\n",
    "        final_dict['filetype'] = uploaded_file.content_type\n",
    "        filename = secure_filename(uploaded_file.filename)\n",
    "        final_dict['filename'] = filename\n",
    "        final_dict['file'] = uploaded_file.read()\n",
    "        file_ext = os.path.splitext(filename)[1].lower()\n",
    "        if file_ext not in app.config['UPLOAD_EXTENSIONS']:\n",
    "            return ('', 204) # 204 no content response\n",
    "            # return flask.send_from_directory('./splash_page/', 'error.html')\n",
    "\n",
    "    # Special tasks if the form is upload_form\n",
    "    if request.form.get('feedback_form_name') == 'upload_form':\n",
    "        uploaded_cif = request.files['cif_file']\n",
    "        cif_filename = secure_filename(uploaded_cif.filename)\n",
    "        file_ext = os.path.splitext(cif_filename)[1].lower()\n",
    "        if file_ext != '.cif':\n",
    "            return ('', 204) # 204 no content response\n",
    "            # return flask.send_from_directory('./splash_page/', 'error.html')\n",
    "        final_dict['cif_file_name'] = cif_filename\n",
    "        final_dict['structure'] = uploaded_cif.read()\n",
    "\n",
    "    final_dict['ip'] = request.remote_addr\n",
    "    final_dict['timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    print(final_dict)\n",
    "    collection.insert(final_dict) # insert the dictionary into the mongodb collection\n",
    "    return ('', 204) # 204 no content response\n",
    "    # return flask.send_from_directory('./splash_page/', 'success.html')\n",
    "\n",
    "## Handle removal request\n",
    "@app.route('/process_removal', methods=['POST'])\n",
    "def process_removal():\n",
    "    \"\"\"\n",
    "    process_removal emails mofsimplify@mit.edu when the removal form is filled out.\n",
    "    \"\"\" \n",
    "\n",
    "    email = request.form.get('email')\n",
    "    comments = request.form.get('comments')\n",
    "    ip = request.remote_addr\n",
    "    timestamp = datetime.now().isoformat()\n",
    "\n",
    "    # grabbing environment variables (should be in .bashrc_conda or .zshrc)\n",
    "    EMAIL_ADDRESS = os.environ.get('EMAIL_USER')\n",
    "    EMAIL_PASSWORD = os.environ.get('EMAIL_PASS')\n",
    "\n",
    "    # https://www.youtube.com/watch?v=JRCJ6RtE3xU\n",
    "    with smtplib.SMTP('smtp.gmail.com', 587) as smtp:\n",
    "        smtp.ehlo()\n",
    "        smtp.starttls()\n",
    "        smtp.ehlo()\n",
    "\n",
    "        # logging in to mail server\n",
    "        smtp.login(EMAIL_ADDRESS, EMAIL_PASSWORD)\n",
    "\n",
    "        subject = 'MOFSimplify removal request'\n",
    "        body = f'email: {email}\\ncomments: {comments}\\nip: {ip}\\ntimestamp: {timestamp}'\n",
    "\n",
    "        msg = f'Subject: {subject}\\n\\n{body}'\n",
    "\n",
    "        smtp.sendmail(EMAIL_ADDRESS, 'mofsimplify@mit.edu', msg) # sending to mofsimplify@mit.edu\n",
    "\n",
    "    return ('', 204) # 204 no content response\n",
    "\n",
    "## Splash page management. Splash page is currently disabled.\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "@app.route('/<path:path>', methods=['GET', 'POST'])\n",
    "def index(path='index.html'):\n",
    "  if request.method == 'POST':\n",
    "    username = 'user1'\n",
    "    # user = User()\n",
    "    # user.id = username\n",
    "    # flask_login.login_user(user)\n",
    "    # This is the section where the password is checked.\n",
    "    # if request.form.get('password') == users[username]['password']:\n",
    "    #   user = User()\n",
    "    #   user.id = 'user1'\n",
    "    #   flask_login.login_user(user)\n",
    "  # print('is user authenticated?')\n",
    "  # print(current_user.is_authenticated)\n",
    "  # print('input check')\n",
    "  # print(request.form.get('password'))\n",
    "  # if current_user.is_authenticated:\n",
    "  #   return flask.send_from_directory('.', 'index.html')\n",
    "  # elif request.form.get('password') == None:\n",
    "  #   return flask.send_from_directory('./splash_page/', path)\n",
    "  # else:\n",
    "  #   return flask.send_from_directory('./splash_page/', 'index_wrong_password.html')\n",
    "  return flask.send_from_directory('.', 'index.html')\n",
    "\n",
    "@app.route('/mof_examples/<path:path>') # needed for fetch\n",
    "def serve_example(path):\n",
    "    \"\"\"\n",
    "    serve_example returns a file to MOFSimplify.\n",
    "    The file is intended to be a cif file of an example MOF. \n",
    "    So, this function serves the example MOF.\n",
    "\n",
    "    :param path: The path to the desired example MOF in the mof_examples folder. For example, HKUST1.cif.\n",
    "    :return: The MOF specified in the path input.\n",
    "    \"\"\" \n",
    "    return flask.send_from_directory('mof_examples', path)\n",
    "\n",
    "@app.route('/how_to_cite.html')\n",
    "def serve_cite():\n",
    "    \"\"\"\n",
    "    serve_cite serves the how to cite page.\n",
    "    So the user is redirected to the how to cite page.\n",
    "\n",
    "    :return: The how to cite page.\n",
    "    \"\"\" \n",
    "    return flask.send_from_directory('.', 'how_to_cite.html')\n",
    "\n",
    "@app.route('/stable_MOFs.html')\n",
    "def serve_stable_bb_page():\n",
    "    \"\"\"\n",
    "    serve_cite serves the stable MOFs page.\n",
    "    So the user is redirected to the stable MOFs page.\n",
    "\n",
    "    :return: The stable MOFs page.\n",
    "    \"\"\" \n",
    "    return flask.send_from_directory('.', 'stable_MOFs.html')\n",
    "\n",
    "@app.route('/libraries/<path:path>')\n",
    "def serve_library_files(path):\n",
    "    \"\"\"\n",
    "    serve_library_files returns a file to MOFSimplify.\n",
    "    The file is intended to be a library file, either .js or .css.\n",
    "\n",
    "    :param path: The path to the desired library in the libraries folder. For example, jquery-3.4.1.min.js.\n",
    "    :return: The library specified in the path input.\n",
    "    \"\"\" \n",
    "    return flask.send_from_directory('libraries', path)\n",
    "\n",
    "@app.route('/list_content/<path:path>')\n",
    "def serve_list_files(path):\n",
    "    \"\"\"\n",
    "    serve_list_files returns a file to MOFSimplify.\n",
    "    The file is intended to be a list file, which contains information for dropdowns.\n",
    "\n",
    "    :param path: The path to the desired library in the libraries folder.\n",
    "    :return: The list file specified in the path input.\n",
    "    \"\"\" \n",
    "    return flask.send_from_directory('list_content', path)\n",
    "\n",
    "@app.route('/bbcif/<path:path>')\n",
    "def serve_bbcif(path):\n",
    "    \"\"\"\n",
    "    serve_bbcif returns a file to MOFSimplify.\n",
    "    The file is intended to be a cif file for a MOF that was constructed using MOFSimplify's building block functionality.\n",
    "    So, this function serves the building block generated MOF.\n",
    "\n",
    "    :param path: The path to the desired MOF in the user's building block folder.\n",
    "    :return: The cif file for the building block generated MOF.\n",
    "    \"\"\" \n",
    "\n",
    "    path_parts = path.split('~')\n",
    "    cif_name = path_parts[0]\n",
    "    user_ID = path_parts[1]\n",
    "    return flask.send_from_directory('temp_file_creation_' + user_ID + '/tobacco_3.0/output_cifs', cif_name);\n",
    "\n",
    "@app.route('/CoRE2019/<path:path>') # needed for fetch\n",
    "def serve_CoRE_MOF(path):\n",
    "    \"\"\"\n",
    "    serve_CoRE_MOF returns a file to MOFSimplify.\n",
    "    The file is intended to be a cif file. It should be a CoRE MOF.\n",
    "\n",
    "    :param path: The path to the desired MOF in the CoRE2019 folder.\n",
    "    :return: The cif file for the neighbor MOF.\n",
    "    \"\"\" \n",
    "    return flask.send_from_directory('CoRE2019', path)\n",
    "\n",
    "@app.route('/unoptimized_geo/<path:path>')\n",
    "def serve_unoptimized_MOF(path):\n",
    "    \"\"\"\n",
    "    serve_unoptimized_MOF returns a file to MOFSimplify.\n",
    "    The file is intended to be a cif file. It should be a MOF.\n",
    "\n",
    "    :param path: The path to the desired MOF in the stable_building_blocks folder.\n",
    "    :return: The cif file for the non geometry optimized MOF.\n",
    "    \"\"\" \n",
    "\n",
    "    MOF_type = type_determination(path)\n",
    "\n",
    "    return flask.send_from_directory(f'stable_building_blocks/initial_structures/{MOF_type}', path)\n",
    "\n",
    "@app.route('/optimized_geo/<path:path>')\n",
    "def serve_optimized_MOF(path):\n",
    "    \"\"\"\n",
    "    serve_optimized_MOF returns a file to MOFSimplify.\n",
    "    The file is intended to be a cif file. It should be a MOF.\n",
    "\n",
    "    :param path: The path to the desired MOF in the stable_building_blocks folder.\n",
    "    :return: The cif file for the geometry optimized MOF.\n",
    "    \"\"\" \n",
    "\n",
    "    MOF_type = type_determination(path)\n",
    "\n",
    "    return flask.send_from_directory(f'stable_building_blocks/optimized_structures/{MOF_type}', 'optimized_' + path)\n",
    "\n",
    "@app.route('/ris_files/MOFSimplify_citation.ris')\n",
    "def serve_ris():\n",
    "    \"\"\"\n",
    "    serve_bbcif returns a file to MOFSimplify.\n",
    "    The file is intended the citation file for the MOFSimplify paper.\n",
    "\n",
    "    :return: The rif citation file.\n",
    "    \"\"\" \n",
    "\n",
    "    return flask.send_from_directory('ris_files', 'MOFSimplify_citation.ris')\n",
    "\n",
    "def listdir_nohidden(path): # used for bb_generate. Ignores hidden files\n",
    "    \"\"\"\n",
    "    listdir_nohidden returns files in the current directory that are not hidden. \n",
    "    It is used as a helper function in the bb_generate function.\n",
    "\n",
    "    :param path: The path to be examined.\n",
    "    :return: The non-hidden files in the current path.\n",
    "    \"\"\" \n",
    "    myList = os.listdir(path);\n",
    "    for i in myList:\n",
    "        if i.startswith('.'):\n",
    "            myList.remove(i)\n",
    "    return myList\n",
    "\n",
    "def file_age_in_seconds(pathname): \n",
    "    \"\"\"\n",
    "    file_age_in_seconds returns the age of the file/folder specified in pathname since the last modification.\n",
    "    It is used as a helper function in the set_ID function.\n",
    "\n",
    "    :return: The age of the file/folder specified in pathname since the last modification, in seconds.\n",
    "    \"\"\" \n",
    "\n",
    "    return time.time() - os.stat(pathname)[stat.ST_MTIME] # time since last modification\n",
    "\n",
    "@app.route('/curr_users', methods=['GET'])\n",
    "def curr_num_users():\n",
    "    \"\"\"\n",
    "    curr_num_users returns the current number of users on MOFSimplify.\n",
    "    This is determined by looking at user specific folders. User specific folders that have not been used for a while are deleted (see the set_ID function). \n",
    "\n",
    "    :return: The number of extant user-specific folders on MOFSimplify.\n",
    "    \"\"\" \n",
    "\n",
    "    sum = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(MOFSIMPLIFY_PATH):\n",
    "        for dir in dirs:\n",
    "            target_str = 'temp_file_creation'\n",
    "            if len(dir) > len(target_str) and target_str in dir: \n",
    "                # target_str in dir since all copies start with temp_file_creation\n",
    "                # len(dir) > len(target_str) to prevent counting the original temp_file_creation folder\n",
    "                sum += 1\n",
    "\n",
    "    return str(sum+1)\n",
    "\n",
    "@app.route('/get_bb_generated_MOF', methods=['POST']) \n",
    "def bb_generate():\n",
    "    \"\"\"\n",
    "    bb_generated generates a MOF using the building blocks and net specified by the user. \n",
    "    The function uses ToBaCCo code, version 3.0. \n",
    "    It returns the constructed MOF's name to the front end\n",
    "\n",
    "    :return: The name of the building block MOF.\n",
    "    \"\"\"\n",
    "\n",
    "    tobacco_folder = MOFSIMPLIFY_PATH + \"temp_file_creation_\" + str(session['ID']) + \"/tobacco_3.0/\"\n",
    "\n",
    "    # Grab data\n",
    "    my_data = json.loads(flask.request.get_data())\n",
    "\n",
    "    linker = my_data['linker']\n",
    "    sbu = my_data['sbu']\n",
    "    net = my_data['net']\n",
    "\n",
    "    # clear the edges, nodes, templates, and output cifs folders to start fresh\n",
    "        # when running python tobacco.py, it looks in these folders\n",
    "\n",
    "    shutil.rmtree(tobacco_folder + 'edges')\n",
    "    os.mkdir(tobacco_folder + 'edges')\n",
    "    shutil.rmtree(tobacco_folder + 'nodes')\n",
    "    os.mkdir(tobacco_folder + 'nodes')\n",
    "    shutil.rmtree(tobacco_folder + 'templates')\n",
    "    os.mkdir(tobacco_folder + 'templates')\n",
    "    shutil.rmtree(tobacco_folder + 'output_cifs')\n",
    "    os.mkdir(tobacco_folder + 'output_cifs')\n",
    "\n",
    "    # copy over the linker, sbu, and net specified by the user in the edges, nodes, and templates folders\n",
    "\n",
    "    shutil.copy(tobacco_folder + 'edges_database/' + linker + '.cif', tobacco_folder + 'edges/' + linker + '.cif')\n",
    "    shutil.copy(tobacco_folder + 'nodes_database/' + sbu + '.cif', tobacco_folder + 'nodes/' + sbu + '.cif')\n",
    "    shutil.copy(tobacco_folder + 'template_database/' + net + '.cif', tobacco_folder + 'templates/' + net + '.cif')\n",
    "\n",
    "\n",
    "    # run the command to construct the MOF\n",
    "    os.chdir(tobacco_folder) \n",
    "    # note: os.chdir here could cause issues if multiple users are using the website and try to make a building block generated MOF at the same time, since MOFSimplify server might chdir when someone else is in the middle of an operation\n",
    "    # luckily, it is a quick operation, so this is unlikely\n",
    "    subprocess.run(['python', 'tobacco.py']) \n",
    "    os.chdir(MOFSIMPLIFY_PATH)\n",
    "\n",
    "    # if successful, there will be an output cif in the folder output_cifs\n",
    "    if listdir_nohidden(tobacco_folder + 'output_cifs') == []: # no files in folder\n",
    "        print('Construction failed.')\n",
    "        return 'FAILED'\n",
    "    \n",
    "    constructed_MOF = listdir_nohidden(tobacco_folder + 'output_cifs')\n",
    "    constructed_MOF = constructed_MOF[0] # getting the first, and only, element out of the list\n",
    "\n",
    "    dictionary = {};\n",
    "\n",
    "    dictionary['mof_name'] = constructed_MOF\n",
    "\n",
    "    # getting the primitive cell using molSimplify\n",
    "    get_primitive(tobacco_folder + 'output_cifs/' + constructed_MOF, tobacco_folder + 'output_cifs/primitive_' + constructed_MOF);\n",
    "\n",
    "    json_object = json.dumps(dictionary, indent = 4);\n",
    "\n",
    "    return json_object\n",
    "\n",
    "\n",
    "### Next, the prediction functions ### \n",
    "\n",
    "\n",
    "def normalize_data_solvent(df_train, df_newMOF, fnames, lname, debug=False):\n",
    "    \"\"\"\n",
    "    normalize_data_solvent takes in two DataFrames df_train and df_newMOF, one for the training data (many rows) and one for the new MOF (one row) for which a prediction is to be generated.\n",
    "    This function also takes in fnames (the feature names) and lname (the target property name).\n",
    "    This function normalizes the X values from the pandas DataFrames and returns them as X_train and X_newMOF.\n",
    "    It also standardizes y_train, which are the solvent removal stability flags in the training data DataFrame, and returns x_scaler (which scaled X_train).\n",
    "        By standardizes, I mean that it makes the values of y_train either 0 or 1\n",
    "\n",
    "    :param df_train: A pandas DataFrame of the training data.\n",
    "    :param df_newMOF: A pandas DataFrame of the new MOF being analyzed.\n",
    "    :param fnames: An array of column names of the descriptors.\n",
    "    :param lname: An array of the column name of the target.\n",
    "    :param debug: A boolean that determines whether extra information is printed.\n",
    "    :return: numpy.ndarray X_train, the descriptors of the training data. Its number of rows is the number of MOFs in the training data. Its number of columns is the number of descriptors.\n",
    "    :return: numpy.ndarray X_newMOF, the descriptors of the new MOF being analyzed by MOFSimplify. It contains only one row.\n",
    "    :return: numpy.ndarray y_train, the solvent removal stabilities of the training data. \n",
    "    :return: sklearn.preprocessing._data.StandardScaler x_scaler, the scaler used to normalize the descriptor data to unit mean and a variance of 1.\n",
    "    \"\"\" \n",
    "    _df_train = df_train.copy().dropna(subset=fnames+lname)\n",
    "    _df_newMOF = df_newMOF.copy().dropna(subset=fnames) \n",
    "    X_train, X_newMOF = _df_train[fnames].values, _df_newMOF[fnames].values # takes care of ensuring ordering is same for both X\n",
    "    y_train = _df_train[lname].values\n",
    "    if debug:\n",
    "        print(\"training data reduced from %d -> %d because of nan.\" % (len(df_train), y_train.shape[0]))\n",
    "    x_scaler = sklearn.preprocessing.StandardScaler()\n",
    "    x_scaler.fit(X_train)\n",
    "    X_train = x_scaler.transform(X_train)\n",
    "    X_newMOF = x_scaler.transform(X_newMOF)\n",
    "    y_train = np.array([1 if x == 1 else 0 for x in y_train.reshape(-1, )])\n",
    "    return X_train, X_newMOF, y_train, x_scaler\n",
    "\n",
    "def standard_labels(df, key=\"flag\"):\n",
    "    \"\"\"\n",
    "    standard_labels makes the solvent removal stability either 1 (stable upon solvent removal) or 0 (unstable upon solvent removal)\n",
    "    \"flag\" is the column under which solvent removal stability is reported in the DataFrame\n",
    "\n",
    "    :param df: A pandas DataFrame to modify.\n",
    "    :param key: The column in the pandas DataFrame to look at.\n",
    "    :return: The modified pandas DataFrame.\n",
    "    \"\"\" \n",
    "    flags = [1 if row[key] == 1 else 0 for _, row in df.iterrows()] # Look through all rows of the DataFrame df.\n",
    "    df[key] = flags\n",
    "    return df\n",
    "\n",
    "def run_solvent_ANN(user_id, path, MOF_name, solvent_ANN):\n",
    "    \"\"\"\n",
    "    run_solvent_ANN runs the solvent removal stability ANN with the desired MOF as input.\n",
    "    It returns a prediction between zero and one. This prediction corresponds to an assessment of MOF stability upon solvent removal.\n",
    "    The further the prediction is from 0.5, the more sure the ANN is.\n",
    "\n",
    "    :param user_id: str, the session ID of the user\n",
    "    :param path: str, the server's path to the MOFSimplify folder on the server\n",
    "    :param MOF_name: str, the name of the MOF for which a prediction is being generated\n",
    "    :param solvent_ANN: keras.engine.training.Model, the ANN itself\n",
    "    :return: str str(new_MOF_pred[0][0]), the model solvent removal stability prediction \n",
    "    :return: list neighbor_names, the latent space nearest neighbor MOFs in the solvent removal stability ANN\n",
    "    :return: list neighbor_distances, the latent space distances of the latent space nearest neighbor MOFs in neighbor_names \n",
    "    \"\"\" \n",
    "\n",
    "    RACs = ['D_func-I-0-all','D_func-I-1-all','D_func-I-2-all','D_func-I-3-all',\n",
    "     'D_func-S-0-all', 'D_func-S-1-all', 'D_func-S-2-all', 'D_func-S-3-all',\n",
    "     'D_func-T-0-all', 'D_func-T-1-all', 'D_func-T-2-all', 'D_func-T-3-all',\n",
    "     'D_func-Z-0-all', 'D_func-Z-1-all', 'D_func-Z-2-all', 'D_func-Z-3-all',\n",
    "     'D_func-chi-0-all', 'D_func-chi-1-all', 'D_func-chi-2-all',\n",
    "     'D_func-chi-3-all', 'D_lc-I-0-all', 'D_lc-I-1-all', 'D_lc-I-2-all',\n",
    "     'D_lc-I-3-all', 'D_lc-S-0-all', 'D_lc-S-1-all', 'D_lc-S-2-all',\n",
    "     'D_lc-S-3-all', 'D_lc-T-0-all', 'D_lc-T-1-all', 'D_lc-T-2-all',\n",
    "     'D_lc-T-3-all', 'D_lc-Z-0-all', 'D_lc-Z-1-all', 'D_lc-Z-2-all',\n",
    "     'D_lc-Z-3-all', 'D_lc-chi-0-all', 'D_lc-chi-1-all', 'D_lc-chi-2-all',\n",
    "     'D_lc-chi-3-all', 'D_mc-I-0-all', 'D_mc-I-1-all', 'D_mc-I-2-all',\n",
    "     'D_mc-I-3-all', 'D_mc-S-0-all', 'D_mc-S-1-all', 'D_mc-S-2-all',\n",
    "     'D_mc-S-3-all', 'D_mc-T-0-all', 'D_mc-T-1-all', 'D_mc-T-2-all',\n",
    "     'D_mc-T-3-all', 'D_mc-Z-0-all', 'D_mc-Z-1-all', 'D_mc-Z-2-all',\n",
    "     'D_mc-Z-3-all', 'D_mc-chi-0-all', 'D_mc-chi-1-all', 'D_mc-chi-2-all',\n",
    "     'D_mc-chi-3-all', 'f-I-0-all', 'f-I-1-all', 'f-I-2-all', 'f-I-3-all',\n",
    "     'f-S-0-all', 'f-S-1-all', 'f-S-2-all', 'f-S-3-all', 'f-T-0-all', 'f-T-1-all',\n",
    "     'f-T-2-all', 'f-T-3-all', 'f-Z-0-all', 'f-Z-1-all', 'f-Z-2-all', 'f-Z-3-all',\n",
    "     'f-chi-0-all', 'f-chi-1-all', 'f-chi-2-all', 'f-chi-3-all', 'f-lig-I-0',\n",
    "     'f-lig-I-1', 'f-lig-I-2', 'f-lig-I-3', 'f-lig-S-0', 'f-lig-S-1', 'f-lig-S-2',\n",
    "     'f-lig-S-3', 'f-lig-T-0', 'f-lig-T-1', 'f-lig-T-2', 'f-lig-T-3', 'f-lig-Z-0',\n",
    "     'f-lig-Z-1', 'f-lig-Z-2', 'f-lig-Z-3', 'f-lig-chi-0', 'f-lig-chi-1',\n",
    "     'f-lig-chi-2', 'f-lig-chi-3', 'func-I-0-all', 'func-I-1-all',\n",
    "     'func-I-2-all', 'func-I-3-all', 'func-S-0-all', 'func-S-1-all',\n",
    "     'func-S-2-all', 'func-S-3-all', 'func-T-0-all', 'func-T-1-all',\n",
    "     'func-T-2-all', 'func-T-3-all', 'func-Z-0-all', 'func-Z-1-all',\n",
    "     'func-Z-2-all', 'func-Z-3-all', 'func-chi-0-all', 'func-chi-1-all',\n",
    "     'func-chi-2-all', 'func-chi-3-all', 'lc-I-0-all', 'lc-I-1-all', 'lc-I-2-all',\n",
    "     'lc-I-3-all', 'lc-S-0-all', 'lc-S-1-all', 'lc-S-2-all', 'lc-S-3-all',\n",
    "     'lc-T-0-all', 'lc-T-1-all', 'lc-T-2-all', 'lc-T-3-all', 'lc-Z-0-all',\n",
    "     'lc-Z-1-all', 'lc-Z-2-all', 'lc-Z-3-all', 'lc-chi-0-all', 'lc-chi-1-all',\n",
    "     'lc-chi-2-all', 'lc-chi-3-all', 'mc-I-0-all', 'mc-I-1-all', 'mc-I-2-all',\n",
    "     'mc-I-3-all', 'mc-S-0-all', 'mc-S-1-all', 'mc-S-2-all', 'mc-S-3-all',\n",
    "     'mc-T-0-all', 'mc-T-1-all', 'mc-T-2-all', 'mc-T-3-all', 'mc-Z-0-all',\n",
    "     'mc-Z-1-all', 'mc-Z-2-all', 'mc-Z-3-all', 'mc-chi-0-all', 'mc-chi-1-all',\n",
    "     'mc-chi-2-all', 'mc-chi-3-all']\n",
    "    geo = ['Df','Di', 'Dif','GPOAV','GPONAV','GPOV','GSA','POAV','POAV_vol_frac',\n",
    "      'PONAV','PONAV_vol_frac','VPOV','VSA','cell_v']\n",
    "     \n",
    "    other = ['cif_file','name','filename']\n",
    "\n",
    "    ANN_path = path + 'model/solvent/ANN/'\n",
    "    temp_file_path = path + 'temp_file_creation_' + user_id + '/'\n",
    "    df_train = pd.read_csv(ANN_path+'dropped_connectivity_dupes/train.csv')\n",
    "    df_train = df_train.loc[:, (df_train != df_train.iloc[0]).any()]\n",
    "    df_newMOF = pd.read_csv(temp_file_path + 'merged_descriptors/' + MOF_name + '_descriptors.csv') # assumes that temp_file_creation/ is in parent folder\n",
    "    features = [val for val in df_train.columns.values if val in RACs+geo]\n",
    "\n",
    "    df_train = standard_labels(df_train, key=\"flag\")\n",
    "\n",
    "    # The normalize_data_solvent function is expecting a DataFrame with each MOF in a separate row, and features in columns\n",
    "\n",
    "    ### Utilize the function below to normalize the RACs + geos of the new MOF\n",
    "    # newMOF refers to the MOF that has been uploaded to MOFSimplify, for which a prediction will be generated\n",
    "    X_train, X_newMOF, y_train, x_scaler = normalize_data_solvent(df_train, df_newMOF, features, [\"flag\"], debug=False)\n",
    "    # Order of values in X_newMOF matters, but this is taken care of in normalize_data_solvent.\n",
    "    X_train.shape, y_train.reshape(-1, ).shape\n",
    "    model = solvent_ANN\n",
    "\n",
    "    from tensorflow.python.keras.backend import set_session\n",
    "    with tf_session.as_default(): # session stuff is needed because the model was loaded from h5 a while ago\n",
    "        with tf_session.graph.as_default():\n",
    "            ### new_MOF_pred will be a decimal value between 0 and 1, below 0.5 is unstable, above 0.5 is stable\n",
    "            new_MOF_pred = np.round(model.predict(X_newMOF),2) # round to 2 decimals\n",
    "\n",
    "            # Define the function for the latent space. This will depend on the model. We want the layer before the last, in this case this was the 12th one.\n",
    "            get_latent = K.function([model.layers[0].input],\n",
    "                                    [model.layers[12].output]) # Last layer before dense-last\n",
    "\n",
    "            # Get the latent vectors for the training data first, then the latent vectors for the test data.\n",
    "            training_latent = get_latent([X_train, 0])[0]\n",
    "            design_latent = get_latent([X_newMOF, 0])[0]\n",
    "\n",
    "    # Compute the pairwise distances between the test latent vectors and the train latent vectors to get latent distances\n",
    "    d1 = pairwise_distances(design_latent,training_latent,n_jobs=30)\n",
    "    df1 = pd.DataFrame(data=d1, columns=df_train['CoRE_name'].tolist())\n",
    "    df1.to_csv(temp_file_path + 'solvent_test_latent_dists.csv')\n",
    "\n",
    "    # Want to find the closest points (let's say the closest 5 points); so, smallest values in df1\n",
    "    neighbors = 5 # number of closest points\n",
    "\n",
    "    # will make arrays of length neighbors, where each entry is the next closest neighbor (will do this for both names and distances)\n",
    "    neighbors_names = []\n",
    "    neighbors_distances = []\n",
    "\n",
    "    df_reformat = df1.min(axis='index')\n",
    "\n",
    "    for i in range(neighbors):\n",
    "        name = df_reformat.idxmin() # name of next closest complex in the training data\n",
    "        distance = df_reformat.min() # distance of the next closest complex in the training data to the new MOF\n",
    "        df_reformat = df_reformat.drop(name) # dropping the next closest complex, in order to find the next-next closest complex\n",
    "\n",
    "        neighbors_names.append(name)\n",
    "        neighbors_distances.append(str(distance))\n",
    "\n",
    "    return str(new_MOF_pred[0][0]), neighbors_names, neighbors_distances\n",
    "\n",
    "\n",
    "def normalize_data_thermal(df_train, df_newMOF, fnames, lname, debug=False): # Function assumes it gets pandas DataFrames with MOFs as rows and features as columns\n",
    "    \"\"\"\n",
    "    normalize_data_thermal takes in two DataFrames df_train and df_newMOF, one for the training data (many rows) and one for the new MOF (one row) for which a prediction is to be generated.\n",
    "    This function also takes in fnames (the feature names) and lname (the target property name).\n",
    "    This function normalizes the X values from the pandas DataFrames and returns them as X_train and X_newMOF.\n",
    "    It also normalizes y_train, which are the thermal breakdown temperatures in the training data DataFrame, and returns x_scaler (which scaled X_train) and y_scaler (which scaled y_train).\n",
    "\n",
    "    :param df_train: A pandas DataFrame of the training data.\n",
    "    :param df_newMOF: A pandas DataFrame of the new MOF being analyzed.\n",
    "    :param fnames: An array of column names of the descriptors.\n",
    "    :param lname: An array of the column name of the target.\n",
    "    :param debug: A boolean that determines whether extra information is printed.\n",
    "    :return: numpy.ndarray X_train, the descriptors of the training data. Its number of rows is the number of MOFs in the training data. Its number of columns is the number of descriptors.\n",
    "    :return: numpy.ndarray X_newMOF, the descriptors of the new MOF being analyzed by MOFSimplify. It contains only one row.\n",
    "    :return: numpy.ndarray y_train, the thermal stabilities of the training data. \n",
    "    :return: sklearn.preprocessing._data.StandardScaler x_scaler, the scaler used to normalize the descriptor data to unit mean and a variance of 1. \n",
    "    :return: sklearn.preprocessing._data.StandardScaler y_scaler, the scaler used to normalize the target data to unit mean and a variance of 1.\n",
    "    \"\"\" \n",
    "    _df_train = df_train.copy().dropna(subset=fnames+lname)\n",
    "    _df_newMOF = df_newMOF.copy().dropna(subset=fnames) \n",
    "    X_train, X_newMOF = _df_train[fnames].values, _df_newMOF[fnames].values # takes care of ensuring ordering is same for both X\n",
    "    y_train = _df_train[lname].values\n",
    "    if debug:\n",
    "        print(\"training data reduced from %d -> %d because of nan.\" % (len(df_train), y_train.shape[0]))\n",
    "    x_scaler = sklearn.preprocessing.StandardScaler()\n",
    "    x_scaler.fit(X_train)\n",
    "    X_train = x_scaler.transform(X_train)\n",
    "    X_newMOF = x_scaler.transform(X_newMOF)\n",
    "    y_scaler = sklearn.preprocessing.StandardScaler()\n",
    "    y_scaler.fit(y_train)\n",
    "    y_train = y_scaler.transform(y_train)\n",
    "    return X_train, X_newMOF, y_train, x_scaler, y_scaler\n",
    "\n",
    "def run_thermal_ANN(user_id, path, MOF_name, thermal_ANN):\n",
    "    \"\"\"\n",
    "    run_thermal_ANN runs the thermal stability ANN with the desired MOF as input.\n",
    "    It returns a prediction for the thermal breakdown temperature of the chosen MOF.\n",
    "\n",
    "    :param user_id: str, the session ID of the user\n",
    "    :param path: str, the server's path to the MOFSimplify folder on the server\n",
    "    :param MOF_name: str, the name of the MOF for which a prediction is being generated\n",
    "    :param thermal_ANN: keras.engine.training.Model, the ANN itself\n",
    "    :return: str new_MOF_pred, the model thermal stability prediction \n",
    "    :return: list neighbor_names, the latent space nearest neighbor MOFs in the thermal stability ANN\n",
    "    :return: list neighbor_distances, the latent space distances of the latent space nearest neighbor MOFs in neighbor_names \n",
    "    \"\"\" \n",
    "\n",
    "    RACs = ['D_func-I-0-all','D_func-I-1-all','D_func-I-2-all','D_func-I-3-all',\n",
    "     'D_func-S-0-all', 'D_func-S-1-all', 'D_func-S-2-all', 'D_func-S-3-all',\n",
    "     'D_func-T-0-all', 'D_func-T-1-all', 'D_func-T-2-all', 'D_func-T-3-all',\n",
    "     'D_func-Z-0-all', 'D_func-Z-1-all', 'D_func-Z-2-all', 'D_func-Z-3-all',\n",
    "     'D_func-chi-0-all', 'D_func-chi-1-all', 'D_func-chi-2-all',\n",
    "     'D_func-chi-3-all', 'D_lc-I-0-all', 'D_lc-I-1-all', 'D_lc-I-2-all',\n",
    "     'D_lc-I-3-all', 'D_lc-S-0-all', 'D_lc-S-1-all', 'D_lc-S-2-all',\n",
    "     'D_lc-S-3-all', 'D_lc-T-0-all', 'D_lc-T-1-all', 'D_lc-T-2-all',\n",
    "     'D_lc-T-3-all', 'D_lc-Z-0-all', 'D_lc-Z-1-all', 'D_lc-Z-2-all',\n",
    "     'D_lc-Z-3-all', 'D_lc-chi-0-all', 'D_lc-chi-1-all', 'D_lc-chi-2-all',\n",
    "     'D_lc-chi-3-all', 'D_mc-I-0-all', 'D_mc-I-1-all', 'D_mc-I-2-all',\n",
    "     'D_mc-I-3-all', 'D_mc-S-0-all', 'D_mc-S-1-all', 'D_mc-S-2-all',\n",
    "     'D_mc-S-3-all', 'D_mc-T-0-all', 'D_mc-T-1-all', 'D_mc-T-2-all',\n",
    "     'D_mc-T-3-all', 'D_mc-Z-0-all', 'D_mc-Z-1-all', 'D_mc-Z-2-all',\n",
    "     'D_mc-Z-3-all', 'D_mc-chi-0-all', 'D_mc-chi-1-all', 'D_mc-chi-2-all',\n",
    "     'D_mc-chi-3-all', 'f-I-0-all', 'f-I-1-all', 'f-I-2-all', 'f-I-3-all',\n",
    "     'f-S-0-all', 'f-S-1-all', 'f-S-2-all', 'f-S-3-all', 'f-T-0-all', 'f-T-1-all',\n",
    "     'f-T-2-all', 'f-T-3-all', 'f-Z-0-all', 'f-Z-1-all', 'f-Z-2-all', 'f-Z-3-all',\n",
    "     'f-chi-0-all', 'f-chi-1-all', 'f-chi-2-all', 'f-chi-3-all', 'f-lig-I-0',\n",
    "     'f-lig-I-1', 'f-lig-I-2', 'f-lig-I-3', 'f-lig-S-0', 'f-lig-S-1', 'f-lig-S-2',\n",
    "     'f-lig-S-3', 'f-lig-T-0', 'f-lig-T-1', 'f-lig-T-2', 'f-lig-T-3', 'f-lig-Z-0',\n",
    "     'f-lig-Z-1', 'f-lig-Z-2', 'f-lig-Z-3', 'f-lig-chi-0', 'f-lig-chi-1',\n",
    "     'f-lig-chi-2', 'f-lig-chi-3', 'func-I-0-all', 'func-I-1-all',\n",
    "     'func-I-2-all', 'func-I-3-all', 'func-S-0-all', 'func-S-1-all',\n",
    "     'func-S-2-all', 'func-S-3-all', 'func-T-0-all', 'func-T-1-all',\n",
    "     'func-T-2-all', 'func-T-3-all', 'func-Z-0-all', 'func-Z-1-all',\n",
    "     'func-Z-2-all', 'func-Z-3-all', 'func-chi-0-all', 'func-chi-1-all',\n",
    "     'func-chi-2-all', 'func-chi-3-all', 'lc-I-0-all', 'lc-I-1-all', 'lc-I-2-all',\n",
    "     'lc-I-3-all', 'lc-S-0-all', 'lc-S-1-all', 'lc-S-2-all', 'lc-S-3-all',\n",
    "     'lc-T-0-all', 'lc-T-1-all', 'lc-T-2-all', 'lc-T-3-all', 'lc-Z-0-all',\n",
    "     'lc-Z-1-all', 'lc-Z-2-all', 'lc-Z-3-all', 'lc-chi-0-all', 'lc-chi-1-all',\n",
    "     'lc-chi-2-all', 'lc-chi-3-all', 'mc-I-0-all', 'mc-I-1-all', 'mc-I-2-all',\n",
    "     'mc-I-3-all', 'mc-S-0-all', 'mc-S-1-all', 'mc-S-2-all', 'mc-S-3-all',\n",
    "     'mc-T-0-all', 'mc-T-1-all', 'mc-T-2-all', 'mc-T-3-all', 'mc-Z-0-all',\n",
    "     'mc-Z-1-all', 'mc-Z-2-all', 'mc-Z-3-all', 'mc-chi-0-all', 'mc-chi-1-all',\n",
    "     'mc-chi-2-all', 'mc-chi-3-all']\n",
    "    geo = ['Df','Di', 'Dif','GPOAV','GPONAV','GPOV','GSA','POAV','POAV_vol_frac',\n",
    "      'PONAV','PONAV_vol_frac','VPOV','VSA','cell_v']\n",
    "     \n",
    "    other = ['cif_file','name','filename']\n",
    "\n",
    "    ANN_path = path + 'model/thermal/ANN/'\n",
    "    temp_file_path = path + 'temp_file_creation_' + user_id + '/'\n",
    "    df_train_all = pd.read_csv(ANN_path+\"train.csv\").append(pd.read_csv(ANN_path+\"val.csv\"))\n",
    "    df_train = pd.read_csv(ANN_path+\"train.csv\")\n",
    "    df_train = df_train.loc[:, (df_train != df_train.iloc[0]).any()]\n",
    "    df_newMOF = pd.read_csv(temp_file_path + 'merged_descriptors/' + MOF_name + '_descriptors.csv') # Assume temp_file_creation/ in parent directory\n",
    "    features = [val for val in df_train.columns.values if val in RACs+geo]\n",
    "\n",
    "    X_train, X_newMOF, y_train, x_scaler, y_scaler = normalize_data_thermal(df_train, df_newMOF, features, [\"T\"], debug=False)\n",
    "    X_train.shape, y_train.reshape(-1, ).shape \n",
    "\n",
    "    model = thermal_ANN\n",
    "\n",
    "    from tensorflow.python.keras.backend import set_session\n",
    "    with tf_session.as_default():\n",
    "        with tf_session.graph.as_default():\n",
    "            new_MOF_pred = y_scaler.inverse_transform(model.predict(X_newMOF))\n",
    "            new_MOF_pred = np.round(new_MOF_pred,1) # round to 1 decimal\n",
    "\n",
    "            # isolating just the prediction, since the model spits out the prediction like [[PREDICTION]], as in, in hard brackets\n",
    "            new_MOF_pred = new_MOF_pred[0][0]\n",
    "            new_MOF_pred = str(new_MOF_pred)\n",
    "\n",
    "            # adding units\n",
    "            degree_sign= u'\\N{DEGREE SIGN}'\n",
    "            new_MOF_pred = new_MOF_pred + degree_sign + 'C' # degrees Celsius\n",
    "\n",
    "            # Define the function for the latent space. This will depend on the model. We want the layer before the last, in this case this was the 8th one.\n",
    "            get_latent = K.function([model.layers[0].input],\n",
    "                                    [model.layers[8].output]) # Last layer before dense-last\n",
    "\n",
    "            # Get the latent vectors for the training data first, then the latent vectors for the test data.\n",
    "            training_latent = get_latent([X_train, 0])[0]\n",
    "            design_latent = get_latent([X_newMOF, 0])[0]\n",
    "\n",
    "            print(training_latent.shape,design_latent.shape)\n",
    "\n",
    "    # Compute the pairwise distances between the test latent vectors and the train latent vectors to get latent distances\n",
    "    d1 = pairwise_distances(design_latent,training_latent,n_jobs=30)\n",
    "    df1 = pd.DataFrame(data=d1, columns=df_train['CoRE_name'].tolist())\n",
    "    df1.to_csv(temp_file_path + 'solvent_test_latent_dists.csv')\n",
    "\n",
    "    # Want to find the closest points (let's say the closest 5 points); so, smallest values in df1\n",
    "    neighbors = 5 # number of closest points\n",
    "\n",
    "    # will make arrays of length neighbors, where each entry is the next closest neighbor (will do this for both names and distances)\n",
    "    neighbors_names = []\n",
    "    neighbors_distances = []\n",
    "\n",
    "    df_reformat = df1.min(axis='index')\n",
    "\n",
    "    for i in range(neighbors):\n",
    "        name = df_reformat.idxmin() # name of next closest complex in the training data\n",
    "        distance = df_reformat.min() # distance of the next closest complex in the training data to the new MOF\n",
    "        df_reformat = df_reformat.drop(name) # dropping the next closest complex, in order to find the next-next closest complex\n",
    "\n",
    "        neighbors_names.append(name)\n",
    "        neighbors_distances.append(str(distance))\n",
    "\n",
    "    return new_MOF_pred, neighbors_names, neighbors_distances\n",
    "\n",
    "\n",
    "def descriptor_generator(name, structure, prediction_type, is_entry):\n",
    "    \"\"\"\n",
    "    # descriptor_generator is used by both ss_predict() and ts_predict() to generate RACs and Zeo++ descriptors.\n",
    "    # These descriptors are subsequently used in ss_predict() and ts_predict() for the ANN models.\n",
    "    # Inputs are the name of the MOF and the structure (cif file text) of the MOF for which descriptors are to be generated.\n",
    "    # The third input indicates the type of prediction (solvent removal or thermal).\n",
    "\n",
    "    :param name: str, the name of the MOF being analyzed.\n",
    "    :param structure: str, the text of the cif file of the MOF being analyzed.\n",
    "    :param prediction_type: str, the type of prediction being run. Can either be 'solvent' or 'thermal'.\n",
    "    :param is_entry: boolean, indicates whether the descriptor CSV has already been written.\n",
    "    :return: Depends, either the string 'FAILED' if descriptor generation fails, a dictionary myDict (if the MOF being analyzed is in the training data), or an array myResult (if the MOF being analyzed is not in the training data) \n",
    "    \"\"\" \n",
    "\n",
    "    print('TIME CHECK 2')\n",
    "    import time # debugging\n",
    "    timeStarted = time.time() # save start time (debugging)\n",
    "\n",
    "    temp_file_folder = MOFSIMPLIFY_PATH + \"temp_file_creation_\" + str(session['ID']) + '/'\n",
    "    cif_folder = temp_file_folder + 'cifs/'\n",
    "\n",
    "    # Write the data back to a cif file.\n",
    "    try:\n",
    "        cif_file = open(cif_folder + name + '.cif', 'w')\n",
    "    except FileNotFoundError:\n",
    "        return 'FAILED'\n",
    "    cif_file.write(structure)\n",
    "    cif_file.close()\n",
    "\n",
    "    # There can be a RACs folder for solvent predictions and a RACs folder for thermal predictions. Same for Zeo++.\n",
    "    RACs_folder = temp_file_folder +  prediction_type + '_RACs/'\n",
    "    zeo_folder = temp_file_folder + prediction_type + '_zeo++/'\n",
    "\n",
    "    # Delete the RACs folder, then remake it (to start fresh for this prediction).\n",
    "    shutil.rmtree(RACs_folder)\n",
    "    os.mkdir(RACs_folder)\n",
    "\n",
    "    # Doing the same with the Zeo++ folder.\n",
    "    shutil.rmtree(zeo_folder)\n",
    "    os.mkdir(zeo_folder)\n",
    "\n",
    "    if not is_entry: # have to generate the CSV\n",
    "\n",
    "        # Next, running MOF featurization\n",
    "        try:\n",
    "            get_primitive(cif_folder + name + '.cif', cif_folder + name + '_primitive.cif');\n",
    "        except ValueError:\n",
    "            return 'FAILED'\n",
    "\n",
    "        timeDelta = time.time() - timeStarted # get execution time\n",
    "        print('Finished process in ' + str(timeDelta) + ' seconds')\n",
    "\n",
    "        print('TIME CHECK 3')\n",
    "\n",
    "        timeStarted = time.time() # save start time (debugging)\n",
    "\n",
    "        # get_MOF_descriptors is used in RAC_getter.py to get RAC features.\n",
    "            # The files that are generated from RAC_getter.py: lc_descriptors.csv, sbu_descriptors.csv, linker_descriptors.csv\n",
    "\n",
    "        # cmd1, cmd2, and cmd3 are for Zeo++. cm4 is for RACs.\n",
    "        cmd1 = MOFSIMPLIFY_PATH + 'zeo++-0.3/network -ha -res ' + zeo_folder + name + '_pd.txt ' + cif_folder + name + '_primitive.cif'\n",
    "        cmd2 = MOFSIMPLIFY_PATH + 'zeo++-0.3/network -sa 1.86 1.86 10000 ' + zeo_folder + name + '_sa.txt ' + cif_folder + name + '_primitive.cif'\n",
    "        cmd3 = MOFSIMPLIFY_PATH + 'zeo++-0.3/network -volpo 1.86 1.86 10000 ' + zeo_folder + name + '_pov.txt '+ cif_folder + name + '_primitive.cif'\n",
    "        cmd4 = 'python ' + MOFSIMPLIFY_PATH + 'model/RAC_getter.py %s %s %s' %(cif_folder, name, RACs_folder)\n",
    "\n",
    "        # four parallelized Zeo++ and RAC commands\n",
    "        process1 = subprocess.Popen(cmd1, stdout=subprocess.PIPE, stderr=None, shell=True)\n",
    "        process2 = subprocess.Popen(cmd2, stdout=subprocess.PIPE, stderr=None, shell=True)\n",
    "        process3 = subprocess.Popen(cmd3, stdout=subprocess.PIPE, stderr=None, shell=True)\n",
    "        process4 = subprocess.Popen(cmd4, stdout=subprocess.PIPE, stderr=None, shell=True)\n",
    "\n",
    "        output1 = process1.communicate()[0]\n",
    "        output2 = process2.communicate()[0]\n",
    "        output3 = process3.communicate()[0]\n",
    "        output4 = process4.communicate()[0]\n",
    "\n",
    "        # Have written output of Zeo++ commands to files. Now, code below extracts information from those files.\n",
    "\n",
    "        ''' \n",
    "        The geometric descriptors are:\n",
    "        - the maximum included sphere (Di)\n",
    "        - maximum free sphere (Df)\n",
    "        - maximum included sphere in the free sphere path (Dif)\n",
    "        - gravimetric pore volume (GPOV)\n",
    "        - volumetric pore volume (VPOV)\n",
    "        - gravimetric surface area (GSA)\n",
    "        - volumetric surface area (VSA)\n",
    "        - cell volume (cell_v)\n",
    "        - gravimetric pore accessible volume (GPOAV)\n",
    "        - gravimetric pore non-accessible volume (GPONAV)\n",
    "        - pore-accessible volume (POAV)\n",
    "        - pore non-accessible volume (PONAV)\n",
    "        - pore accessible void fraction (POAVF)\n",
    "        - pore nonaccessible void fraction (PONAVF)\n",
    "\n",
    "        All Zeo++ calculations use a probe radius of 1.86 angstrom, and zeo++ is called by subprocess.\n",
    "        '''\n",
    "\n",
    "        dict_list = []\n",
    "        cif_file = name + '_primitive.cif' \n",
    "        basename = cif_file.strip('.cif')\n",
    "        largest_included_sphere, largest_free_sphere, largest_included_sphere_along_free_sphere_path = np.nan, np.nan, np.nan\n",
    "        unit_cell_volume, crystal_density, VSA, GSA = np.nan, np.nan, np.nan, np.nan\n",
    "        VPOV, GPOV = np.nan, np.nan\n",
    "        POAV, PONAV, GPOAV, GPONAV, POAV_volume_fraction, PONAV_volume_fraction = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "        if (os.path.exists(zeo_folder + name + '_pd.txt') & os.path.exists(zeo_folder + name + '_sa.txt') &\n",
    "            os.path.exists(zeo_folder + name + '_pov.txt')):\n",
    "            with open(zeo_folder + name + '_pd.txt') as f:\n",
    "                pore_diameter_data = f.readlines()\n",
    "                for row in pore_diameter_data:\n",
    "                    largest_included_sphere = float(row.split()[1]) # largest included sphere\n",
    "                    largest_free_sphere = float(row.split()[2]) # largest free sphere\n",
    "                    largest_included_sphere_along_free_sphere_path = float(row.split()[3]) # largest included sphere along free sphere path\n",
    "            with open(zeo_folder + name + '_sa.txt') as f:\n",
    "                surface_area_data = f.readlines()\n",
    "                for i, row in enumerate(surface_area_data):\n",
    "                    if i == 0:\n",
    "                        unit_cell_volume = float(row.split('Unitcell_volume:')[1].split()[0]) # unit cell volume\n",
    "                        crystal_density = float(row.split('Density:')[1].split()[0]) # crystal density\n",
    "                        VSA = float(row.split('ASA_m^2/cm^3:')[1].split()[0]) # volumetric surface area\n",
    "                        GSA = float(row.split('ASA_m^2/g:')[1].split()[0]) # gravimetric surface area\n",
    "            with open(zeo_folder + name + '_pov.txt') as f:\n",
    "                pore_volume_data = f.readlines()\n",
    "                for i, row in enumerate(pore_volume_data):\n",
    "                    if i == 0:\n",
    "                        density = float(row.split('Density:')[1].split()[0])\n",
    "                        POAV = float(row.split('POAV_A^3:')[1].split()[0]) # Probe accessible pore volume\n",
    "                        PONAV = float(row.split('PONAV_A^3:')[1].split()[0]) # Probe non-accessible probe volume\n",
    "                        GPOAV = float(row.split('POAV_cm^3/g:')[1].split()[0])\n",
    "                        GPONAV = float(row.split('PONAV_cm^3/g:')[1].split()[0])\n",
    "                        POAV_volume_fraction = float(row.split('POAV_Volume_fraction:')[1].split()[0]) # probe accessible volume fraction\n",
    "                        PONAV_volume_fraction = float(row.split('PONAV_Volume_fraction:')[1].split()[0]) # probe non accessible volume fraction\n",
    "                        VPOV = POAV_volume_fraction+PONAV_volume_fraction\n",
    "                        GPOV = VPOV/density\n",
    "        else:\n",
    "            print('Not all 3 files exist, so at least one Zeo++ call failed!', 'sa: ',os.path.exists(zeo_folder + name + '_sa.txt'), \n",
    "                  '; pd: ',os.path.exists(zeo_folder + name + '_pd.txt'), '; pov: ', os.path.exists(zeo_folder + name + '_pov.txt'))\n",
    "            return 'FAILED'\n",
    "        geo_dict = {'name':basename, 'cif_file':cif_file, 'Di':largest_included_sphere, 'Df': largest_free_sphere, 'Dif': largest_included_sphere_along_free_sphere_path,\n",
    "                    'cell_v': unit_cell_volume, 'VSA':VSA, 'GSA': GSA, 'VPOV': VPOV, 'GPOV':GPOV, 'POAV_vol_frac':POAV_volume_fraction, \n",
    "                    'PONAV_vol_frac':PONAV_volume_fraction, 'GPOAV':GPOAV,'GPONAV':GPONAV,'POAV':POAV,'PONAV':PONAV}\n",
    "        dict_list.append(geo_dict)\n",
    "        geo_df = pd.DataFrame(dict_list)\n",
    "        geo_df.to_csv(zeo_folder + 'geometric_parameters.csv',index=False)\n",
    "\n",
    "        # error handling for cmd4\n",
    "        with open(RACs_folder + 'RAC_getter_log.txt', 'r') as f:\n",
    "            if f.readline() == 'FAILED':\n",
    "                print('RAC generation failed.')\n",
    "                return 'FAILED'\n",
    "\n",
    "\n",
    "        timeDelta = time.time() - timeStarted # get execution time\n",
    "        print('Finished process in ' + str(timeDelta) + ' seconds')\n",
    "\n",
    "        print('TIME CHECK 4')\n",
    "\n",
    "        timeStarted = time.time() # save start time\n",
    "\n",
    "        # Merging geometric information with the RAC information that is in the get_MOF_descriptors-generated files (lc_descriptors.csv, sbu_descriptors.csv, linker_descriptors.csv)\n",
    "        try:\n",
    "            lc_df = pd.read_csv(RACs_folder + \"lc_descriptors.csv\") \n",
    "            sbu_df = pd.read_csv(RACs_folder + \"sbu_descriptors.csv\")\n",
    "            linker_df = pd.read_csv(RACs_folder + \"linker_descriptors.csv\")\n",
    "        except Exception: # csv files have been deleted\n",
    "            return 'FAILED' \n",
    "\n",
    "        lc_df = lc_df.mean().to_frame().transpose() # averaging over all rows. Convert resulting Series into a DataFrame, then transpose\n",
    "        sbu_df = sbu_df.mean().to_frame().transpose()\n",
    "        linker_df = linker_df.mean().to_frame().transpose()\n",
    "\n",
    "        merged_df = pd.concat([geo_df, lc_df, sbu_df, linker_df], axis=1)\n",
    "\n",
    "        merged_df.to_csv(temp_file_folder + '/merged_descriptors/' + name + '_descriptors.csv',index=False) # written in /temp_file_creation_SESSIONID\n",
    "\n",
    "    else: # CSV is already written\n",
    "        merged_df = pd.read_csv(temp_file_folder + '/merged_descriptors/' + name + '_descriptors.csv')\n",
    "\n",
    "    if prediction_type == 'solvent':\n",
    "\n",
    "        ANN_folder = MOFSIMPLIFY_PATH + 'model/solvent/ANN/'\n",
    "        train_df = pd.read_csv(ANN_folder + 'dropped_connectivity_dupes/train.csv')\n",
    "\n",
    "    if prediction_type == 'thermal':\n",
    "        ANN_folder = MOFSIMPLIFY_PATH + 'model/thermal/ANN/'\n",
    "        train_df = pd.read_csv(ANN_folder + 'train.csv')\n",
    "\n",
    "    ### Here, I do a check to see if the current MOF is in the training data. ###\n",
    "    # If it is, then I return the known truth for the MOF, rather than make a prediction.\n",
    "\n",
    "    # Will iterate through the rows of the train pandas DataFrame\n",
    "\n",
    "    in_train = False\n",
    "\n",
    "    for index, row in train_df.iterrows(): # iterate through rows of the training data MOFs\n",
    "\n",
    "        row_match = True # gets set to false if any values don't match \n",
    "        matching_MOF = None # gets assigned a value if the current MOF is in the training data\n",
    "\n",
    "        for col in merged_df.columns: # iterate through columns of the single new MOF we are predicting on (merged_df is just one row)\n",
    "            if col == 'name' or col == 'cif_file' or col == 'Dif':\n",
    "                continue # skip these\n",
    "                # Dif was sometimes differing between new Zeo++ call and training data value, for the same MOF\n",
    "\n",
    "            # If for any property a training MOF and the new MOF we are predicting on differ too much, we know they are not the same MOF\n",
    "            # So row_match is set to false for this training MOF\n",
    "            if np.absolute(row[col] - merged_df.iloc[0][col]) > 0.05 * np.absolute(merged_df.iloc[0][col]): # row[col] != merged_df.iloc[0][col] was leading to some same values being identified as different b/c of some floating 10^-15 values \n",
    "                row_match = False\n",
    "                break\n",
    "        \n",
    "        if row_match and prediction_type == 'solvent': # all columns for the row match! Some training MOF is the same as the new MOF\n",
    "            in_train = True\n",
    "            match_truth = row['flag'] # the flag for the MOF that matches the current MOF\n",
    "            matching_MOF = row['CoRE_name']\n",
    "            print(row['CoRE_name'])\n",
    "            break\n",
    "        if row_match and prediction_type =='thermal': # all columns for the row match! Some training MOF is the same as the new MOF\n",
    "            in_train = True\n",
    "            match_truth = row['T'] # the flag for the MOF that matches the current MOF\n",
    "            match_truth = np.round(match_truth,1) # round to 1 decimal\n",
    "            matching_MOF = row['CoRE_name']\n",
    "            # adding units\n",
    "            degree_sign= u'\\N{DEGREE SIGN}'\n",
    "            match_truth = str(match_truth) + degree_sign + 'C' # degrees Celsius\n",
    "            break\n",
    "\n",
    "    if in_train:\n",
    "        myDict = {'in_train': True, 'truth': match_truth, 'match': matching_MOF}\n",
    "        return myDict\n",
    "\n",
    "    ### End of training data check. ###\n",
    "\n",
    "    timeDelta = time.time() - timeStarted # get execution time\n",
    "    print('Finished process in ' + str(timeDelta) + ' seconds')\n",
    "\n",
    "    print('TIME CHECK 5')\n",
    "\n",
    "    myResult = [temp_file_folder, ANN_folder]\n",
    "\n",
    "    return myResult \n",
    "    \n",
    "@app.route('/predict_solvent_stability', methods=['POST']) \n",
    "def ss_predict():\n",
    "    \"\"\"\n",
    "    ss_predict generates the solvent removal stability prediction for the selected MOF.\n",
    "        Or it will return a ground truth if the MOF is in the solvent stability ANN training data.\n",
    "        Or it will return a signal that descriptor generation failed and thus a prediction cannot be made. \n",
    "    RAC featurization and Zeo++ geometry information for the selected MOF is generated, using descriptor_generator.\n",
    "    Then, Aditya's model is applied to make a prediction using run_solvent_ANN.\n",
    "\n",
    "    If the structure is already in our history.MOFSimplify collection, we use information from the database to speed things up.\n",
    "\n",
    "    :return: dict results, contains the prediction and information on latent space nearest neighbors.\n",
    "        May instead return a dictionary output if the MOF is in the training data, containing the ground truth and the name of the matching MOF in the training data\n",
    "        May instead return a string 'FAILED' if descriptor generation fails.\n",
    "        May instead return a string 'OVERLOAD' if the server is being asked too much of, in order to avoid 50X errors (like 504, aka Gateway Time-out).\n",
    "    \"\"\" \n",
    "\n",
    "    global operation_counter # global variable\n",
    "    global last_operation_counter_clear\n",
    "\n",
    "    import time \n",
    "    print('quick time check')\n",
    "    print(time.time())\n",
    "    print(last_operation_counter_clear)\n",
    "    print(time.time() - last_operation_counter_clear)\n",
    "    print('end of quick time check')\n",
    "    if time.time() - last_operation_counter_clear > 300: # 5 minutes or more since last time operation_counter was zero'd\n",
    "        print(f'operation_counter cleared at {time.time()}')\n",
    "        last_operation_counter_clear = time.time()\n",
    "        operation_counter = 0\n",
    "\n",
    "    print(f'Current operation_counter: {operation_counter}')\n",
    "\n",
    "    if operation_counter >= MAX_OPERATIONS:\n",
    "        return 'OVERLOAD'\n",
    "\n",
    "    operation_counter += 1\n",
    "\n",
    "    print('TIME CHECK 1')\n",
    "    import time\n",
    "    timeStarted = time.time() # save start time (debugging)\n",
    "\n",
    "    # Grab data and tweak the MOF name.\n",
    "    my_data = json.loads(flask.request.get_data())\n",
    "    structure = my_data['structure']\n",
    "    name = my_data['name']\n",
    "    if name[-4:] == '.cif':\n",
    "        name = name[:-4] # remove the .cif part of the name\n",
    "\n",
    "    timeDelta = time.time() - timeStarted # get execution time\n",
    "    print('Finished process in ' + str(timeDelta) + ' seconds')\n",
    "\n",
    "    temp_file_folder = MOFSIMPLIFY_PATH + \"temp_file_creation_\" + str(session['ID']) + '/'\n",
    "\n",
    "    ### Check in MongoDB history.MOFSimplify collection to see if this structure has been predicted on before. \n",
    "    # Comment out this section if you are running MOFSimplify on your computer, and define is_entry as False. Also select \"No\" for the question May MOFSimplify store information on your MOFs?\n",
    "    client = MongoClient('18.18.63.68',27017) # connect to mongodb. The first argument is the IP address. The second argument is the port.\n",
    "    db = client.history # The history database\n",
    "    collection = db.MOFSimplify_v2 # The MOFSimplify_v2 collection in the history database.\n",
    "        # The collection client.history.MOFSimplify, or db.MOFSimplify, was for a prior version, where the cell_v feature was erroneously called rho\n",
    "    my_documents = collection.find({'structure':structure})\n",
    "    is_entry = my_documents.count() # will be zero or one, depending if structure is in the database\n",
    "    print(f'is_entry is {is_entry}')\n",
    "    if is_entry: # return statements in this if statement are of same form as those from the workflow if the structure isn't in the database. The frontend needs to receive information of the same form.\n",
    "        entry_data = my_documents[0]\n",
    "        if entry_data['failure'] == True: # MOF was not featurizable\n",
    "            print('entry_data[\"failure\"] is True!')\n",
    "            if session['permission']:\n",
    "                db_push_lite(structure=structure, prediction_type='solvent_stability_prediction') # add 1 to the s_times in the database\n",
    "            operation_counter = conditional_diminish(operation_counter)\n",
    "            return 'FAILED'\n",
    "        else: \n",
    "            # Making the csv; can skip csv making in descriptor_generator with this\n",
    "            csv_content = entry_data['csv_content']\n",
    "            with open(temp_file_folder + 'merged_descriptors/' + name + '_descriptors.csv', 'w') as f:\n",
    "                f.write(csv_content)\n",
    "\n",
    "        if entry_data['s_intrain'] == True:\n",
    "            my_dict = {'in_train':True, 'truth':entry_data['s_result'],'match':None} # don't care about the MOF that matches in CoRE here\n",
    "            if session['permission']:\n",
    "                db_push_lite(structure=structure, prediction_type='solvent_stability_prediction') # add 1 to the s_times in the database\n",
    "            operation_counter = conditional_diminish(operation_counter)\n",
    "            return my_dict\n",
    "        elif entry_data['s_intrain'] == False:\n",
    "            my_dict = {'prediction':entry_data['s_result'],'neighbor_names':entry_data['s_neighbornames'],'neighbor_distances':entry_data['s_neighbordists'],'in_train':False}\n",
    "            if session['permission']:\n",
    "                db_push_lite(structure=structure, prediction_type='solvent_stability_prediction') # add 1 to the s_times in the database\n",
    "            operation_counter = conditional_diminish(operation_counter)\n",
    "            return my_dict\n",
    "\n",
    "        # if haven't returned anything by now and entered the if is_entry statement, have a featurizable MOF for which a solvent prediction has not been run, but a thermal prediction has\n",
    "\n",
    "    ###\n",
    "\n",
    "    output = descriptor_generator(name, structure, 'solvent', is_entry) # generate descriptors\n",
    "    if output == 'FAILED': # Description generation failure\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "        if session['permission']:\n",
    "            db_push(structure=structure, prediction_type='solvent_stability_prediction', in_train='', result='', neighbor_names='', neighbor_dists='', failure=True, csv_content='')\n",
    "        return 'FAILED'\n",
    "    elif isinstance(output, dict): # MOF was in the training data. Return the ground truth.\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "\n",
    "        if session['permission']:\n",
    "            # Getting the contents of the CSV containing features.\n",
    "            descriptors_folder = temp_file_folder + \"merged_descriptors/\"\n",
    "            with open(descriptors_folder + name + '_descriptors.csv', 'r') as f:\n",
    "                csv_contents = f.read()\n",
    "\n",
    "            db_push(structure=structure, prediction_type='solvent_stability_prediction', in_train=True, result=output['truth'], neighbor_names='', neighbor_dists='', failure=False, csv_content=csv_contents)\n",
    "        return output\n",
    "    else: # grabbing some variables (that aren't used lol). We will make a prediction\n",
    "        temp_file_folder = output[0]\n",
    "        ANN_folder = output[1]\n",
    "\n",
    "    # Applying the model next\n",
    "\n",
    "    timeStarted = time.time() # save start time (debugging)\n",
    "    prediction, neighbor_names, neighbor_distances = run_solvent_ANN(str(session['ID']), MOFSIMPLIFY_PATH, name, solvent_model)\n",
    "\n",
    "    results = {'prediction': prediction,\n",
    "        'neighbor_names': neighbor_names, # The names of the five nearest latent space nearest neighbors in the ANN's training data. The ANNs used CoRE MOFs as training data.\n",
    "        'neighbor_distances': neighbor_distances, # The latent space distances of the five nearest latent space neighbors.\n",
    "        'in_train': False} # A prediction was made. The requested (selected) MOF was not in the training data.\n",
    "\n",
    "    timeDelta = time.time() - timeStarted # get execution time\n",
    "    print('Finished process in ' + str(timeDelta) + ' seconds') \n",
    "\n",
    "    print('TIME CHECK 6')\n",
    "\n",
    "    if session['permission']:\n",
    "        # database push of MOF structure and ANN results\n",
    "\n",
    "        # Getting the contents of the CSV containing features.\n",
    "        descriptors_folder = temp_file_folder + \"merged_descriptors/\"\n",
    "        with open(descriptors_folder + name + '_descriptors.csv', 'r') as f:\n",
    "            csv_contents = f.read()\n",
    "\n",
    "        db_push(structure=structure, prediction_type='solvent_stability_prediction', in_train=False, result=prediction, neighbor_names=neighbor_names, neighbor_dists=neighbor_distances, failure=False, csv_content=csv_contents)\n",
    "\n",
    "    operation_counter = conditional_diminish(operation_counter)\n",
    "    return results\n",
    "\n",
    "@app.route('/predict_thermal_stability', methods=['POST']) \n",
    "def ts_predict():\n",
    "    \"\"\"\n",
    "    ts_predict generates the thermal stability prediction for the selected MOF.\n",
    "        Or it will return a ground truth if the MOF is in the thermal stability ANN training data.\n",
    "        Or it will return a signal that descriptor generation failed and thus a prediction cannot be made. \n",
    "    RAC featurization and Zeo++ geometry information for the selected MOF is generated, using descriptor_generator.\n",
    "    Then, Aditya's model is applied to make a prediction using run_thermal_ANN.\n",
    "\n",
    "    If the structure is already in our history.MOFSimplify collection, we use information from the database to speed things up.\n",
    "\n",
    "    :return: dict results, contains the prediction and information on latent space nearest neighbors.\n",
    "        May instead return a dictionary output if the MOF is in the training data, containing the ground truth and the name of the matching MOF in the training data\n",
    "        May instead return a string 'FAILED' if descriptor generation fails.\n",
    "        May instead return a string 'OVERLOAD' if the server is being asked too much of, in order to avoid 50X errors (like 504, aka Gateway Time-out).\n",
    "    \"\"\" \n",
    "\n",
    "    global operation_counter # global variable\n",
    "    global last_operation_counter_clear\n",
    "\n",
    "    import time \n",
    "    print('quick time check')\n",
    "    print(time.time())\n",
    "    print(last_operation_counter_clear)\n",
    "    print(time.time() - last_operation_counter_clear)\n",
    "    print('end of quick time check')\n",
    "    if time.time() - last_operation_counter_clear > 300: # 5 minutes or more since last time operation_counter was zero'd\n",
    "        print(f'operation_counter cleared at {time.time()}')\n",
    "        last_operation_counter_clear = time.time()\n",
    "        operation_counter = 0\n",
    "\n",
    "    print(f'Current operation_counter: {operation_counter}')\n",
    "\n",
    "    if operation_counter >= MAX_OPERATIONS:\n",
    "        return 'OVERLOAD'\n",
    "\n",
    "    operation_counter += 1\n",
    "\n",
    "    # Grab data and tweak the MOF name.\n",
    "    my_data = json.loads(flask.request.get_data()) # This is a dictionary.\n",
    "    structure = my_data['structure']\n",
    "    name = my_data['name']\n",
    "    if name[-4:] == '.cif':\n",
    "        name = name[:-4] # remove the .cif part of the name\n",
    "\n",
    "    temp_file_folder = MOFSIMPLIFY_PATH + \"temp_file_creation_\" + str(session['ID']) + '/'\n",
    "\n",
    "    ### Check in MongoDB history.MOFSimplify collection to see if this structure has been predicted on before. \n",
    "    # Comment out this section if you are running MOFSimplify on your computer, and define is_entry as False. Also select \"No\" for the question May MOFSimplify store information on your MOFs? \n",
    "    client = MongoClient('18.18.63.68',27017) # connect to mongodb. The first argument is the IP address. The second argument is the port.\n",
    "    db = client.history # The history database\n",
    "    collection = db.MOFSimplify_v2 # The MOFSimplify_v2 collection in the history database.\n",
    "    my_documents = collection.find({'structure':structure})\n",
    "    is_entry = my_documents.count() # will be zero or one, depending if structure is in the database\n",
    "    if is_entry: # return statements in this if statement are of same form as those from the workflow if the structure isn't in the database. The frontend needs to receive information of the same form.\n",
    "        entry_data = my_documents[0]\n",
    "        if entry_data['failure'] == True: # MOF was not featurizable\n",
    "            if session['permission']:\n",
    "                db_push_lite(structure=structure, prediction_type='thermal_stability_prediction') # add 1 to the t_times in the database\n",
    "            operation_counter = conditional_diminish(operation_counter)\n",
    "            return 'FAILED'\n",
    "        else:\n",
    "            # Making the csv; can skip csv making in descriptor_generator with this\n",
    "            csv_content = entry_data['csv_content']\n",
    "            with open(temp_file_folder + 'merged_descriptors/' + name + '_descriptors.csv', 'w') as f:\n",
    "                f.write(csv_content)\n",
    "        if entry_data['t_intrain'] == True:\n",
    "            my_dict = {'in_train':True, 'truth':entry_data['t_result'],'match':None} # don't care about the MOF that matches in CoRE here\n",
    "            if session['permission']:\n",
    "                db_push_lite(structure=structure, prediction_type='thermal_stability_prediction') # add 1 to the t_times in the database\n",
    "            operation_counter = conditional_diminish(operation_counter)\n",
    "            return my_dict\n",
    "        elif entry_data['t_intrain'] == False:\n",
    "            my_dict = {'prediction':entry_data['t_result'],'neighbor_names':entry_data['t_neighbornames'],'neighbor_distances':entry_data['t_neighbordists'],'in_train':False}\n",
    "            if session['permission']:\n",
    "                db_push_lite(structure=structure, prediction_type='thermal_stability_prediction') # add 1 to the t_times in the database\n",
    "            operation_counter = conditional_diminish(operation_counter)\n",
    "            return my_dict\n",
    "\n",
    "    ###\n",
    "\n",
    "    output = descriptor_generator(name, structure, 'thermal', is_entry) # generate descriptors\n",
    "    if output == 'FAILED': # Description generation failure\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "        if session['permission']:\n",
    "            db_push(structure=structure, prediction_type='thermal_stability_prediction', in_train='', result='', neighbor_names='', neighbor_dists='', failure=True, csv_content='')\n",
    "        return 'FAILED'\n",
    "    elif isinstance(output, dict): # MOF was in the training data. Return the ground truth.\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "\n",
    "        if session['permission']:\n",
    "            # Getting the contents of the CSV containing features.\n",
    "            descriptors_folder = temp_file_folder + \"merged_descriptors/\"\n",
    "            with open(descriptors_folder + name + '_descriptors.csv', 'r') as f:\n",
    "                csv_contents = f.read()\n",
    "\n",
    "            db_push(structure=structure, prediction_type='thermal_stability_prediction', in_train=True, result=output['truth'], neighbor_names='', neighbor_dists='', failure=False, csv_content=csv_contents)\n",
    "        return output\n",
    "    else: # grabbing some variables (that aren't used lol). We will make a prediction\n",
    "        temp_file_folder = output[0]\n",
    "        ANN_folder = output[1]\n",
    "\n",
    "    # Applying the model next\n",
    "\n",
    "    timeStarted = time.time() # save start time (debugging)\n",
    "\n",
    "    prediction, neighbor_names, neighbor_distances = run_thermal_ANN(str(session['ID']), MOFSIMPLIFY_PATH, name, thermal_model)\n",
    "\n",
    "    timeDelta = time.time() - timeStarted # get execution time\n",
    "    print('Finished process in ' + str(timeDelta) + ' seconds')\n",
    "\n",
    "    results = {'prediction': prediction,\n",
    "        'neighbor_names': neighbor_names, # The names of the five nearest latent space nearest neighbors in the ANN's training data. The ANNs used CoRE MOFs as training data.\n",
    "        'neighbor_distances': neighbor_distances, # The latent space distances of the five nearest latent space neighbors.\n",
    "        'in_train': False} # A prediction was made. The requested (selected) MOF was not in the training data.\n",
    "\n",
    "    print('TIME CHECK 6 ' + str(session['ID']))\n",
    "\n",
    "    if session['permission']:\n",
    "        # database push of MOF structure and ANN results\n",
    "\n",
    "        # Getting the contents of the CSV containing features.\n",
    "        descriptors_folder = temp_file_folder + \"merged_descriptors/\"\n",
    "        with open(descriptors_folder + name + '_descriptors.csv', 'r') as f:\n",
    "            csv_contents = f.read()\n",
    "\n",
    "        db_push(structure=structure, prediction_type='thermal_stability_prediction', in_train=False, result=prediction, neighbor_names=neighbor_names, neighbor_dists=neighbor_distances, failure=False, csv_content=csv_contents)\n",
    "\n",
    "    operation_counter = conditional_diminish(operation_counter)\n",
    "    return results\n",
    "\n",
    "def db_push(structure, prediction_type, in_train, result, neighbor_names, neighbor_dists, failure, csv_content):\n",
    "    \"\"\"\n",
    "    # db_push sends the structure of the MOF being predicted on, and other information, to the MOFSimplify collection in the MongoDB history database.\n",
    "\n",
    "    :param structure: str, the text of the cif file of the MOF being analyzed.\n",
    "    :param prediction_type: str, the type of prediction being run. Can either be 'solvent_stability_prediction' or 'thermal_stability_prediction'.\n",
    "    :param in_train: boolean, indicates whether the current structure is in the training data or not.\n",
    "    :param result: float, the ground truth or prediction for this structure.\n",
    "    :param neighbor_names: list, the latent space nearest neighbor MOFs in the thermal stability ANN.\n",
    "    :param neighbor_distances: list, the latent space distances of the latent space nearest neighbor MOFs in neighbor_names.\n",
    "    :param failure: boolean, whether the featurization failed.\n",
    "    :param csv_content: string, the content of the csv of features.\n",
    "    :return: A 204 no content response, so the front end does not display a page different than the one it is on.\n",
    "    \"\"\" \n",
    "\n",
    "    print(f'Have entered the db_push function')\n",
    "\n",
    "    client = MongoClient('18.18.63.68',27017) # connect to mongodb\n",
    "    # The first argument is the IP address. The second argument is the port.\n",
    "    db = client.history # The history database\n",
    "    collection = db.MOFSimplify_v2 # The MOFSimplify_v2 collection in the history database.\n",
    "    # s refers to a solvent removal stability prediction; t refers to a thermal stability prediction\n",
    "    fields = ['structure', 's_intrain', 's_result', 's_neighbornames', 's_neighbordists', 't_intrain', 't_result', 't_neighbornames', 't_neighbordists', 'failure', 'csv_content', 's_times', 't_times'] \n",
    "        # s_times and t_times indicate the number of times this structure has had its solvent removal stability or thermal stability predicted\n",
    "    final_dict = {}\n",
    "    for field in fields:\n",
    "        final_dict[field] = '' # start with everything empty\n",
    "\n",
    "    # https://docs.mongodb.com/manual/reference/method/db.collection.update/\n",
    "    my_documents = collection.find({'structure':structure})\n",
    "    is_entry = my_documents.count() # will be zero or one, depending if structure is in the database\n",
    "\n",
    "    final_dict['structure'] = structure\n",
    "    final_dict['failure'] = failure\n",
    "    final_dict['csv_content'] = csv_content\n",
    "    if is_entry == 0: # structure has not been seen before. Make a new entry\n",
    "        # Populate certain fields\n",
    "        if prediction_type == 'solvent_stability_prediction':\n",
    "            final_dict['s_intrain'] = in_train\n",
    "            final_dict['s_result'] = result\n",
    "            final_dict['s_neighbornames'] = neighbor_names\n",
    "            final_dict['s_neighbordists'] = neighbor_dists\n",
    "            final_dict['s_times'] = 1\n",
    "            final_dict['t_times'] = 0\n",
    "        elif prediction_type == 'thermal_stability_prediction':\n",
    "            final_dict['t_intrain'] = in_train\n",
    "            final_dict['t_result'] = result\n",
    "            final_dict['t_neighbornames'] = neighbor_names\n",
    "            final_dict['t_neighbordists'] = neighbor_dists\n",
    "            final_dict['s_times'] = 0\n",
    "            final_dict['t_times'] = 1\n",
    "\n",
    "        # print(final_dict)\n",
    "        collection.insert(final_dict) # insert the dictionary into the mongodb collection\n",
    "    else:  # existing structure. Update existing document (couldn't get actual update function to work, so I delete entry and write a new one.)\n",
    "        my_document = my_documents[0]\n",
    "        final_dict['s_intrain'] = my_document['s_intrain']\n",
    "        final_dict['s_result'] = my_document['s_result']\n",
    "        final_dict['s_neighbornames'] = my_document['s_neighbornames']\n",
    "        final_dict['s_neighbordists'] = my_document['s_neighbordists']\n",
    "        final_dict['t_intrain'] = my_document['t_intrain']\n",
    "        final_dict['t_result'] = my_document['t_result']\n",
    "        final_dict['t_neighbornames'] = my_document['t_neighbornames']\n",
    "        final_dict['t_neighbordists'] = my_document['t_neighbordists']\n",
    "        final_dict['s_times'] = my_document['s_times']\n",
    "        final_dict['t_times'] = my_document['t_times']\n",
    "\n",
    "        if prediction_type == 'solvent_stability_prediction': # structure has had a thermal prediction but not a solvent prediction\n",
    "            final_dict['s_intrain'] = in_train\n",
    "            final_dict['s_result'] = result\n",
    "            final_dict['s_neighbornames'] = neighbor_names\n",
    "            final_dict['s_neighbordists'] = neighbor_dists\n",
    "            final_dict['s_times'] = final_dict['s_times'] + 1\n",
    "        elif prediction_type == 'thermal_stability_prediction': # structure has had a solvent prediction but not a thermal prediction\n",
    "            final_dict['t_intrain'] = in_train\n",
    "            final_dict['t_result'] = result\n",
    "            final_dict['t_neighbornames'] = neighbor_names\n",
    "            final_dict['t_neighbordists'] = neighbor_dists\n",
    "            final_dict['t_times'] = final_dict['t_times'] + 1\n",
    "\n",
    "        collection.remove({'structure':structure}) # delete entry\n",
    "        collection.insert(final_dict) # insert the dictionary into the mongodb collection (so, write new entry)\n",
    "    return ('', 204) # 204 no content response\n",
    "\n",
    "def db_push_lite(structure, prediction_type):\n",
    "    \"\"\"\n",
    "    # db_push_lite increases either s_times or t_times in the document corresponding to structure, in the MOFSimplify collection in the MongoDB history database.\n",
    "    # The prediction_type determines whether s_times or t_times is increased by one.\n",
    "\n",
    "    :param structure: str, the text of the cif file of the MOF being analyzed.\n",
    "    :param prediction_type: str, the type of prediction being run. Can either be 'solvent_stability_prediction' or 'thermal_stability_prediction'.\n",
    "    :return: A 204 no content response, so the front end does not display a page different than the one it is on.\n",
    "    \"\"\" \n",
    "\n",
    "    print('Entering db_push_lite!')\n",
    "\n",
    "    client = MongoClient('18.18.63.68',27017) # connect to mongodb\n",
    "    # The first argument is the IP address. The second argument is the port.\n",
    "    db = client.history # The history database\n",
    "    collection = db.MOFSimplify_v2 # The MOFSimplify_v2 collection in the history database.\n",
    "    final_dict = {}\n",
    "\n",
    "    # https://docs.mongodb.com/manual/reference/method/db.collection.update/\n",
    "    my_documents = collection.find({'structure':structure})\n",
    "\n",
    "    # if db_push_lite is called, structure passed to it will already have a document in the history.MOFSimplify collection\n",
    "\n",
    "    #Update existing document (couldn't get actual update function to work, so I delete entry and write a new one.)\n",
    "    # s refers to a solvent removal stability prediction; t refers to a thermal stability prediction\n",
    "    # s_times and t_times indicate the number of times this structure has had its solvent removal stability or thermal stability predicted\n",
    "    my_document = my_documents[0]\n",
    "    final_dict['structure'] = structure\n",
    "    final_dict['s_intrain'] = my_document['s_intrain']\n",
    "    final_dict['s_result'] = my_document['s_result']\n",
    "    final_dict['s_neighbornames'] = my_document['s_neighbornames']\n",
    "    final_dict['s_neighbordists'] = my_document['s_neighbordists']\n",
    "    final_dict['t_intrain'] = my_document['t_intrain']\n",
    "    final_dict['t_result'] = my_document['t_result']\n",
    "    final_dict['t_neighbornames'] = my_document['t_neighbornames']\n",
    "    final_dict['t_neighbordists'] = my_document['t_neighbordists']\n",
    "    final_dict['s_times'] = my_document['s_times']\n",
    "    final_dict['t_times'] = my_document['t_times']\n",
    "    final_dict['failure'] = my_document['failure']\n",
    "    final_dict['csv_content'] = my_document['csv_content']\n",
    "\n",
    "    if prediction_type == 'solvent_stability_prediction': \n",
    "        print('db_push_lite solvent hit')\n",
    "        final_dict['s_times'] += 1\n",
    "    elif prediction_type == 'thermal_stability_prediction': \n",
    "        print('db_push_lite solvent hit')\n",
    "        final_dict['t_times'] += 1\n",
    "\n",
    "    collection.remove({'structure':structure}) # delete entry\n",
    "    collection.insert(final_dict) # insert the dictionary into the mongodb collection (so, write new entry)\n",
    "    return ('', 204) # 204 no content response\n",
    "\n",
    "@app.route('/plot_thermal_stability', methods=['POST']) \n",
    "def plot_thermal_stability():\n",
    "    \"\"\"\n",
    "    plot_thermal_stability returns a plot of the distribution of thermal breakdown temperatures of the MOFs our ANN was trained on.\n",
    "    Additionally, it displays the position of the current MOF's predicted or ground truth thermal breakdown temperature.\n",
    "\n",
    "    :return: HTML document, a plot of the kernel-density estimate of the breakdown temperatures in the training data \n",
    "        with a red dot indicating the position of the current MOF's predicted or ground truth thermal breakdown temperature.\n",
    "    \"\"\" \n",
    "\n",
    "    # Grab data\n",
    "    info = json.loads(flask.request.get_data()) \n",
    "    my_data = info['temperature'] # this is the current MOF's predicted thermal breakdown temperature\n",
    "    if info['temperature_data_type'] == 'string':\n",
    "        my_data = my_data[:-3] # getting rid of the celsius symbol, left with just the number\n",
    "        my_data = float(my_data)\n",
    "\n",
    "    # Getting the temperature data\n",
    "    temps_df = pd.read_csv(MOFSIMPLIFY_PATH + \"model/thermal/ANN/adjusted_TSD_df_all.csv\")\n",
    "\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg') # noninteractive backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.close(\"all\")\n",
    "    plt.rcParams.update({'font.size': 16}) # large font\n",
    "    import scipy.stats as stats\n",
    "\n",
    "    # In training data, smallest T breakdown is 35, and largest T breakdown is 654.\n",
    "\n",
    "    # Use stats.gaussian_kde to estimate the probability density function from the histogram. This is a kernel-density estimate using Gaussian kernels.\n",
    "    density = stats.gaussian_kde(temps_df['T'])\n",
    "    x = np.arange(30,661,1) # in training data, smallest T breakdown is 35, and largest T breakdown is 654\n",
    "    fig = plt.figure(figsize=(10, 6), dpi=80) # set figure size\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    plt.plot(x, density(x))\n",
    "    plt.plot(my_data, density(my_data), \"or\") # The current MOF's predicted or ground truth thermal breakdown temperature.\n",
    "\n",
    "    ax.set_xlabel('Breakdown temperature (°C)')\n",
    "    ax.set_ylabel('Frequency in the training data')\n",
    "\n",
    "    # The title of the plot differs slightly depending on if the selected MOF is in the training data or not.\n",
    "    if info['prediction']: # MOF wasn't in training data, and its ANN predicted breakdown temperature is used.\n",
    "        ax.set_title('Current MOF\\'s predicted breakdown temperature relative to others')\n",
    "    else: # MOF was in the training data, and its reported breakdown temperature is used.\n",
    "        ax.set_title('Current MOF\\'s breakdown temperature relative to others')\n",
    "\n",
    "    import mpld3\n",
    "\n",
    "    return mpld3.fig_to_html(fig)\n",
    "\n",
    "@app.route('/thermal_stability_percentile', methods=['POST']) \n",
    "def thermal_stability_percentile():\n",
    "    \"\"\"\n",
    "    thermal_stability_percentile returns what percentile the thermal breakdown temperature (prediction or ground truth) of the selected MOF lies in\n",
    "        with respect to the MOFs used to train the ANN for thermal stability predictions.\n",
    "    For example, a 30th percentile breakdown temperature is higher than 30% of the breakdown temperatures in the CoRE training data for the thermal stability ANN.\n",
    "\n",
    "    :return: str our_percentile, the percentile of the thermal breakdown temperature.\n",
    "    \"\"\" \n",
    "\n",
    "    # Grab data.\n",
    "    my_data = json.loads(flask.request.get_data()) # this is the current MOF's predicted thermal breakdown temperature\n",
    "    my_data = my_data[:-3] # getting rid of the Celsius symbol, left with just the number\n",
    "    my_data = float(my_data)\n",
    "\n",
    "    # Getting the temperature data.\n",
    "    temps_df = pd.read_csv(MOFSIMPLIFY_PATH + \"model/thermal/ANN/adjusted_TSD_df_all.csv\")\n",
    "\n",
    "    # Will find what percentile our prediction belongs to, by checking the 100 percentiles and seeing which is closest to our prediction.\n",
    "    difference = np.Infinity\n",
    "\n",
    "    breakdown_Ts = temps_df['T']\n",
    "    for i in np.arange(0,100.1,1): # 0,1, ..., 99, 100\n",
    "        current_percentile = np.percentile(breakdown_Ts, i) # ith percentile\n",
    "        current_difference = np.absolute(my_data - current_percentile) # absolute difference\n",
    "        if current_difference < difference:\n",
    "            difference = current_difference\n",
    "            our_percentile = i\n",
    "\n",
    "    our_percentile = int(our_percentile) # no decimal points\n",
    "    our_percentile = str(our_percentile)\n",
    "\n",
    "    return our_percentile\n",
    "\n",
    "### Helper functions for TGA_plot. ###\n",
    "\n",
    "# I use https://stackoverflow.com/questions/3252194/numpy-and-line-intersections to get code for the intersection of the two TGA lines\n",
    "# for use in /TGA_plot call. perp and seg_intersect are from that stackoverflow page\n",
    "\n",
    "#\n",
    "# line segment intersection using vectors\n",
    "# see Computer Graphics by F.S. Hill\n",
    "#\n",
    "def perp( a ) :\n",
    "    b = np.empty_like(a)\n",
    "    b[0] = -a[1]\n",
    "    b[1] = a[0]\n",
    "    return b\n",
    "\n",
    "# line segment a given by endpoints a1, a2\n",
    "# line segment b given by endpoints b1, b2\n",
    "# return \n",
    "def seg_intersect(a1,a2, b1,b2) :\n",
    "    da = a2-a1\n",
    "    db = b2-b1\n",
    "    dp = a1-b1\n",
    "    dap = perp(da)\n",
    "    denom = np.dot( dap, db)\n",
    "    num = np.dot( dap, dp )\n",
    "    return (num / denom.astype(float))*db + b1\n",
    "\n",
    "### End of helper functions for TGA_plot. ###\n",
    "\n",
    "@app.route('/TGA_plot', methods=['POST'])\n",
    "def TGA_plot():\n",
    "    \"\"\"\n",
    "    TGA_plot makes the TGA plot for the current thermal ANN nearest neighbor.\n",
    "    The TGA plot contains the four points that constitute the simplified TGA data. These points are yellow stars.\n",
    "    The two points representing the flatter part of the trace are connected with a line (line A), \n",
    "        and the two points representing the steeper part of the trace are connected with a line (line B).\n",
    "    The intersection of lines A and B is designated with a red dot.\n",
    "    Lines A and B extend past the red dot for a bit. Lines A and B are blue dashed lines.\n",
    "    The simplified TGA plot is sent to the front end (index.html) for display.\n",
    "\n",
    "    :return: HTML document, the simplified TGA plot.\n",
    "    \"\"\" \n",
    "\n",
    "    # Grab data\n",
    "    my_data = json.loads(flask.request.get_data()); # This is the neighbor complex's name.\n",
    "\n",
    "    # cut off these endings, in order to access the TGA file correctly:\n",
    "    # _clean, _ion_b, _neutral_b, _SL, _charged, _clean_h, _manual, _auto, _charged, etc\n",
    "    cut_index = my_data.find('_') # gets index of the first underscore\n",
    "    my_data = my_data[:cut_index] \n",
    "\n",
    "    # Grab data \n",
    "    slopes_df = pd.read_csv(MOFSIMPLIFY_PATH + \"TGA/raw_TGA_digitization_data/digitized_csv/\" + my_data + \".csv\")\n",
    "\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    for i in range(4): # 0, 1, 2, 3\n",
    "        x_values.append(slopes_df.iloc[[i]]['T (degrees C)'][i])\n",
    "        y_values.append(slopes_df.iloc[[i]]['mass (arbitrary units)'][i])\n",
    "\n",
    "    # Making the four points.\n",
    "    p1 = np.array( [x_values[0], y_values[0]] )\n",
    "    p2 = np.array( [x_values[1], y_values[1]] )\n",
    "\n",
    "    p3 = np.array( [x_values[2], y_values[2]] )\n",
    "    p4 = np.array( [x_values[3], y_values[3]] )\n",
    "\n",
    "    intersection_point = seg_intersect(p1, p2, p3, p4)\n",
    "\n",
    "    # Want lines to extend beyond intersection point. I will make it 20%.\n",
    "    x_extension = [None] * 2 # This variable will hold the x values of the two extension points.\n",
    "    y_extension = [None] * 2 # This variable will hold the y values of the two extension points.\n",
    "    slope = [None] * 2 # This variable will hold the slopes of the two lines.\n",
    "\n",
    "    x_extension[0] = intersection_point[0] + (intersection_point[0] - x_values[0])*0.2 # x_values[0] is the smallest value, x_values[3] is the largest value\n",
    "    x_extension[1] = intersection_point[0] - (x_values[3] - intersection_point[0])*0.2\n",
    "    slope[0] = (y_values[1] - y_values[0])/(x_values[1] - x_values[0])\n",
    "    slope[1] = (y_values[3] - y_values[2])/(x_values[3] - x_values[2])\n",
    "    y_extension[0] = intersection_point[1] + slope[0] * (x_extension[0] - intersection_point[0]) # y2 = y1 + m(x2-x1)\n",
    "    y_extension[1] = intersection_point[1] + slope[1] * (x_extension[1] - intersection_point[0]) # y2 = y1 + m(x2-x1)\n",
    "\n",
    "    # Instantiating the figure object. \n",
    "    graph = figure(title = \"Simplified literature TGA plot of selected thermal ANN neighbor\")  \n",
    "         \n",
    "    # The points to be plotted.\n",
    "    xs = [[x_values[0], x_values[1],intersection_point[0], x_extension[0]], [x_values[2], x_values[3],intersection_point[0], x_extension[1]]] \n",
    "    ys = [[y_values[0], y_values[1],intersection_point[1], y_extension[0]], [y_values[2], y_values[3],intersection_point[1], y_extension[1]]] \n",
    "        \n",
    "    # Plotting the graph.\n",
    "    graph.multi_line(xs, ys, line_dash='dashed') \n",
    "    graph.circle([intersection_point[0]], [intersection_point[1]], size=20, fill_color=\"red\", line_color='black')\n",
    "    graph.star(x=x_values, y=y_values, size=20, fill_color=\"yellow\", line_color='black') # four stars\n",
    "    graph.xaxis.axis_label = 'Temperature (°C)'\n",
    "    graph.yaxis.axis_label = 'Percentage mass remaining or Mass'    \n",
    "\n",
    "    return file_html(graph,CDN,'my plot')\n",
    "\n",
    "@app.route('/get_components', methods=['POST']) \n",
    "def get_components():\n",
    "    \"\"\"\n",
    "    get_components uses Aditya's MOF code to get components (linkers and sbus).\n",
    "    It returns a dictionary with the linker and sbu xyz files' text, along with information about the number of linkers and sbus.\n",
    "    The dictionary also contains the SMILES string for each of the linkers and sbus.\n",
    "\n",
    "    :return: str json_object, encodes a dictionary. The dictionary contains the xyz file geometry information for each component, \n",
    "        the number of each type of component, the unique linkers and sbus (kept track of by their index, see linker_num and sbu_num in the code),\n",
    "        and the SMILES string for each component.\n",
    "\n",
    "        May instead return a string 'OVERLOAD' if the server is being asked too much of, in order to avoid 50X errors (like 504, aka Gateway Time-out).\n",
    "    \"\"\" \n",
    "\n",
    "    global operation_counter # global variable\n",
    "    global last_operation_counter_clear\n",
    "\n",
    "    import time \n",
    "    print('quick time check')\n",
    "    print(time.time())\n",
    "    print(last_operation_counter_clear)\n",
    "    print(time.time() - last_operation_counter_clear)\n",
    "    print('end of quick time check')\n",
    "    if time.time() - last_operation_counter_clear > 300: # 5 minutes or more since last time operation_counter was zero'd\n",
    "        print(f'operation_counter cleared at {time.time()}')\n",
    "        last_operation_counter_clear = time.time()\n",
    "        operation_counter = 0\n",
    "\n",
    "    print(f'Current operation_counter: {operation_counter}')\n",
    "\n",
    "    if operation_counter >= MAX_OPERATIONS:\n",
    "        return 'OVERLOAD'\n",
    "\n",
    "    operation_counter += 1\n",
    "\n",
    "    # Grab data\n",
    "    my_data = json.loads(flask.request.get_data());\n",
    "\n",
    "    structure = my_data['structure']\n",
    "    name = my_data['name']\n",
    "    if name[-4:] == '.cif':\n",
    "        name = name[:-4] # remove the .cif part of the name\n",
    "\n",
    "    temp_file_folder = MOFSIMPLIFY_PATH + \"temp_file_creation_\" + str(session['ID']) + '/'\n",
    "    cif_folder = temp_file_folder + 'cifs/'\n",
    "\n",
    "    # Write the data back to a cif file.\n",
    "    try:\n",
    "        cif_file = open(cif_folder + name + '.cif', 'w')\n",
    "    except FileNotFoundError:\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "        return 'FAILED'\n",
    "    cif_file.write(structure)\n",
    "    cif_file.close()\n",
    "\n",
    "    RACs_folder = temp_file_folder +  'components_RACs/'\n",
    "\n",
    "    # Delete the RACs folder, then remake it (to start fresh for this prediction).\n",
    "    shutil.rmtree(RACs_folder)\n",
    "    os.mkdir(RACs_folder)\n",
    "\n",
    "    # Next, running MOF featurization\n",
    "    try:\n",
    "        get_primitive(cif_folder + name + '.cif', cif_folder + name + '_primitive.cif');\n",
    "    except ValueError:\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "        return 'FAILED'\n",
    "\n",
    "    try:\n",
    "        full_names, full_descriptors = get_MOF_descriptors(cif_folder + name + '_primitive.cif',3,path= RACs_folder, xyzpath= RACs_folder + name + '.xyz');\n",
    "            # makes the linkers and sbus folders, complete with linkers and sbus. We don't care about full_names and full_descriptors in this function\n",
    "    except ValueError:\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "        return 'FAILED'\n",
    "    except NotImplementedError:\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "        return 'FAILED'\n",
    "    except AssertionError:\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "        return 'FAILED'\n",
    "\n",
    "    if (len(full_names) <= 1) and (len(full_descriptors) <= 1): # this is a featurization check from MOF_descriptors.py\n",
    "        operation_counter = conditional_diminish(operation_counter)\n",
    "        return 'FAILED'\n",
    "\n",
    "    # At this point, have the RAC featurization. \n",
    "\n",
    "    # will return a json object (dictionary)\n",
    "    # The fields are string representations of the linkers and sbus, however many there are.\n",
    "\n",
    "    dictionary = {};\n",
    "    \n",
    "    linker_num = 0;\n",
    "    while True:\n",
    "        if not os.path.exists(RACs_folder + 'linkers/' + name + '_primitive_linker_' + str(linker_num) + '.xyz'):\n",
    "            break\n",
    "        else:\n",
    "            linker_file = open(RACs_folder + 'linkers/' + name + '_primitive_linker_' + str(linker_num) + '.xyz', 'r');\n",
    "            linker_info = linker_file.read();\n",
    "            linker_file.close();\n",
    "\n",
    "            dictionary['linker_' + str(linker_num)] = linker_info;\n",
    "\n",
    "            linker_num = linker_num + 1;\n",
    "\n",
    "\n",
    "    sbu_num = 0;\n",
    "    while True:\n",
    "        if not os.path.exists(RACs_folder + 'sbus/' + name + '_primitive_sbu_' + str(sbu_num) + '.xyz'):\n",
    "            break\n",
    "        else:\n",
    "            sbu_file = open(RACs_folder + 'sbus/' + name + '_primitive_sbu_' + str(sbu_num) + '.xyz', 'r');\n",
    "            sbu_info = sbu_file.read();\n",
    "            sbu_file.close();\n",
    "\n",
    "            dictionary['sbu_' + str(sbu_num)] = sbu_info;\n",
    "\n",
    "            sbu_num = sbu_num + 1;\n",
    "\n",
    "\n",
    "    dictionary['total_linkers'] = linker_num;\n",
    "    dictionary['total_sbus'] = sbu_num;\n",
    "\n",
    "\n",
    "    # Identifying which linkers and sbus have different connectivities.\n",
    "\n",
    "    # Code below uses molecular graph determinants.\n",
    "    # two ligands that are the same (via connectivity) will have the same molecular graph determinant. \n",
    "    # Molecular graph determinant fails to distinguish between isomers, but so would RCM (Reverse Cuthill McKee).\n",
    "\n",
    "    # This simple script is meant to be run within the linkers directory, and it will give a bunch of numbers. \n",
    "    # If those numbers are the same, the linker is the same, if not, the linkers are different, etc.\n",
    "\n",
    "    from molSimplify.Classes.mol3D import mol3D\n",
    "    MOF_of_interest = name + '_primitive'\n",
    "    XYZs = sorted(glob.glob(RACs_folder + 'linkers/*'+MOF_of_interest+'*xyz'))\n",
    "    det_list = []\n",
    "\n",
    "    for xyz in XYZs:\n",
    "        net = xyz.replace('xyz', 'net') # substring replacement; getting the appropriate .net file\n",
    "        linker_mol = mol3D()\n",
    "        linker_mol.readfromxyz(xyz)\n",
    "        with open(net,'r') as f:\n",
    "            data = f.readlines()\n",
    "        for i, line in enumerate(data):\n",
    "            if i==0:\n",
    "                # Skip first line\n",
    "                continue\n",
    "            elif i == 1:\n",
    "                graph = np.array([[int(val) for val in line.strip('\\n').split(',')]])\n",
    "            else:\n",
    "                graph = np.append(graph, [np.array([int(val) for val in line.strip('\\n').split(',')])],axis=0)\n",
    "        linker_mol.graph = graph\n",
    "        safedet = linker_mol.get_mol_graph_det(oct=False)\n",
    "        det_list.append(safedet)\n",
    "\n",
    "    #### Linkers with the same molecular graph determinant are the same.\n",
    "    #### Molecular graph determinant does not catch isomers.\n",
    "    linker_det_list = det_list\n",
    "\n",
    "    unique_linker_det = set(linker_det_list) # getting the unique determinants\n",
    "    unique_linker_indices = []\n",
    "    for item in unique_linker_det:\n",
    "        unique_linker_indices.append(linker_det_list.index(item)) # indices of the unique linkers in the list of linkers\n",
    "\n",
    "    XYZs = sorted(glob.glob(RACs_folder + 'sbus/*'+MOF_of_interest+'*xyz'))\n",
    "    det_list = []\n",
    "    for xyz in XYZs:\n",
    "        net = xyz.replace('xyz', 'net') # substring replacement; getting the appropriate .net file\n",
    "        linker_mol = mol3D()\n",
    "        linker_mol.readfromxyz(xyz)\n",
    "        with open(net,'r') as f:\n",
    "            data = f.readlines()\n",
    "        for i, line in enumerate(data):\n",
    "            if i==0:\n",
    "                # Skip first line\n",
    "                continue\n",
    "            elif i == 1:\n",
    "                graph = np.array([[int(val) for val in line.strip('\\n').split(',')]])\n",
    "            else:\n",
    "                graph = np.append(graph, [np.array([int(val) for val in line.strip('\\n').split(',')])],axis=0)\n",
    "        linker_mol.graph = graph\n",
    "        safedet = linker_mol.get_mol_graph_det(oct=False)\n",
    "        det_list.append(safedet)\n",
    "\n",
    "    sbu_det_list = det_list\n",
    "\n",
    "    unique_sbu_det = set(sbu_det_list) # getting the unique determinants\n",
    "    unique_sbu_indices = []\n",
    "    for item in unique_sbu_det:\n",
    "        unique_sbu_indices.append(sbu_det_list.index(item)) # indices of the unique sbus in the list of linkers\n",
    "\n",
    "    # adding the unique indices to the dictionary\n",
    "    dictionary['unique_linker_indices'] = unique_linker_indices\n",
    "    dictionary['unique_sbu_indices'] = unique_sbu_indices\n",
    "\n",
    "\n",
    "    # In this next section, getting the SMILES strings for all of the linkers and sbus using pybel.\n",
    "    # Write the smiles strings to file, then read the file.\n",
    "    import pybel\n",
    "\n",
    "    linkers_folder = RACs_folder + 'linkers/'\n",
    "\n",
    "    for i in range(dictionary['total_linkers']): # 0, 1, 2, ..., numberoflinkersminus1\n",
    "        smilesFile = pybel.Outputfile('smi', linkers_folder + name + '_primitive_linker_' + str(i) + '.txt') # smi refers to SMILES\n",
    "        smilesFile.write(next(pybel.readfile('xyz', linkers_folder + name + '_primitive_linker_' + str(i) + '.xyz'))) # writes SMILES string to the text file\n",
    "\n",
    "        # Next, get the SMILES string from the text file.\n",
    "        f = open(linkers_folder + name + '_primitive_linker_' + str(i) + '.txt', 'r')\n",
    "        line = f.readline()\n",
    "        line = line.split('\\t') # split at tabs\n",
    "        smiles_ID = line[0]\n",
    "        dictionary['linker_' + str(i) + '_SMILES'] = smiles_ID\n",
    "        f.close()\n",
    "\n",
    "    sbus_folder = RACs_folder + 'sbus/'\n",
    "\n",
    "    for i in range(dictionary['total_sbus']):\n",
    "        smilesFile = pybel.Outputfile('smi', sbus_folder + name + '_primitive_sbu_' + str(i) + '.txt') # smi refers to SMILES\n",
    "        smilesFile.write(next(pybel.readfile('xyz', sbus_folder + name + '_primitive_sbu_' + str(i) + '.xyz'))) # writes SMILES string to the text file\n",
    "\n",
    "        # Next, get the SMILES string from the text file.\n",
    "        f = open(sbus_folder + name + '_primitive_sbu_' + str(i) + '.txt', 'r')\n",
    "        line = f.readline()\n",
    "        line = line.split('\\t') # split at tabs\n",
    "        smiles_ID = line[0]\n",
    "        dictionary['sbu_' + str(i) + '_SMILES'] = smiles_ID\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    json_object = json.dumps(dictionary, indent = 4);\n",
    "\n",
    "    operation_counter = conditional_diminish(operation_counter)\n",
    "    return json_object\n",
    "\n",
    "@app.route('/solvent_neighbor_flag', methods=['POST']) \n",
    "def is_stable():\n",
    "    \"\"\"\n",
    "    is_stable returns the flag (i.e. the stability upon solvent removal, yes or no) and DOI of the neighbor sent over from the front end.\n",
    "    The flag is returned to the front end (index.html). \n",
    "\n",
    "    :return: dict myDict, contains the solvent removal stability of the latent space nearest neighbor MOF, and its DOI.\n",
    "    \"\"\" \n",
    "\n",
    "    # Grab data.\n",
    "    my_data = json.loads(flask.request.get_data()); # This is the neighbor complex's name.\n",
    "\n",
    "    print(my_data)\n",
    "    \n",
    "    my_folder = MOFSIMPLIFY_PATH + 'model/solvent/ANN/dropped_connectivity_dupes/'\n",
    "    solvent_flags_df = pd.read_csv(my_folder + 'train.csv')\n",
    "\n",
    "    this_neighbor = solvent_flags_df[solvent_flags_df['CoRE_name'] == my_data] # getting the row with the MOF of interest\n",
    "\n",
    "    this_neighbor_flag = this_neighbor['flag'] # getting the flag value\n",
    "    this_neighbor_flag = str(this_neighbor_flag.iloc[0]) # extract the flag value and return it as a string\n",
    "\n",
    "    # next, getting DOI\n",
    "    this_neighbor_doi = this_neighbor['doi'] # getting the doi\n",
    "    this_neighbor_doi = this_neighbor_doi.iloc[0]\n",
    "\n",
    "    myDict = {'flag': this_neighbor_flag, 'doi': this_neighbor_doi}\n",
    "\n",
    "    return myDict\n",
    "\n",
    "@app.route('/thermal_neighbor_T', methods=['POST']) \n",
    "def breakdown_T():\n",
    "    \"\"\"\n",
    "    breakdown_T returns the thermal breakdown temperature and DOI of the neighbor sent over from the front end.\n",
    "    The information is returned to the front end (index.html).\n",
    "    \n",
    "    :return: dict myDict, contains the breakdown temperature of the latent space nearest neighbor MOF, and its DOI.\n",
    "    \"\"\" \n",
    "\n",
    "    # Grab data\n",
    "    my_data = json.loads(flask.request.get_data()); # This is the neighbor complex's name.\n",
    "\n",
    "    print(my_data)\n",
    "    \n",
    "    # To begin, always go to main directory \n",
    "    ANN_folder = MOFSIMPLIFY_PATH + 'model/thermal/ANN/'\n",
    "\n",
    "    breakdown_T_df = pd.read_csv(ANN_folder + 'train.csv')\n",
    "    this_neighbor = breakdown_T_df[breakdown_T_df['CoRE_name'] == my_data] # getting the row with the MOF of interest\n",
    "    this_neighbor_T = this_neighbor['T'] # getting the breakdown temperature value\n",
    "    this_neighbor_T = str(round(this_neighbor_T.iloc[0], 1)) # extract the breakdown temperature value and return it as a string. Want just one decimal place\n",
    "\n",
    "\n",
    "    TGA_folder = MOFSIMPLIFY_PATH + 'TGA/'\n",
    "    TGA_df = pd.read_excel(TGA_folder + 'TGA_info_log.xlsx')\n",
    "    this_neighbor = TGA_df[TGA_df['CoRE_name'] == my_data] # getting the row with the MOF of interest\n",
    "    this_neighbor_doi = this_neighbor['doi'] # getting the doi\n",
    "    this_neighbor_doi = this_neighbor_doi.iloc[0]\n",
    "\n",
    "    myDict = {'T': this_neighbor_T, 'doi': this_neighbor_doi}\n",
    "\n",
    "    return myDict\n",
    "\n",
    "@app.route('/neighbor_writer', methods=['POST']) \n",
    "def neighbor_writer():\n",
    "    \"\"\"\n",
    "    neighbor_writer writes information to a txt file about the selected latent space nearest neighbor.\n",
    "    It returns the information in that txt file to the front end for downloading.\n",
    "\n",
    "    :return: dict myDict, contains the content for the text file to be downloaded, and the name of the text file to be downloaded.\n",
    "    \"\"\" \n",
    "\n",
    "    # Grab data\n",
    "    my_data = json.loads(flask.request.get_data()); # This is a dictionary with information about the neighbor and the MOF that was analyzed by an ANN\n",
    "\n",
    "    prediction_type = my_data['prediction_type']\n",
    "    current_MOF = my_data['current_MOF']\n",
    "    selected_neighbor = my_data['selected_neighbor']\n",
    "    latent_space_distance = my_data['latent_space_distance']\n",
    "\n",
    "    # next, getting neighbor_truth\n",
    "    if prediction_type == 'solvent':\n",
    "        my_folder = MOFSIMPLIFY_PATH + 'model/solvent/ANN/dropped_connectivity_dupes/'\n",
    "        solvent_flags_df = pd.read_csv(my_folder + 'train.csv')\n",
    "        this_neighbor = solvent_flags_df[solvent_flags_df['CoRE_name'] == selected_neighbor] # getting the row with the MOF of interest\n",
    "        this_neighbor_flag = this_neighbor['flag'] # getting the flag value\n",
    "        neighbor_truth = str(this_neighbor_flag.iloc[0]) # extract the flag value and convert it into a string\n",
    "        if neighbor_truth == '1':\n",
    "            neighbor_truth = 'Stable upon solvent removal'\n",
    "        else:\n",
    "            neighbor_truth = 'Unstable upon solvent removal'\n",
    "\n",
    "    else: # thermal stability\n",
    "        my_folder = MOFSIMPLIFY_PATH + 'model/thermal/ANN/'\n",
    "        breakdown_T_df = pd.read_csv(my_folder + 'train.csv')\n",
    "        this_neighbor = breakdown_T_df[breakdown_T_df['CoRE_name'] == selected_neighbor] # getting the row with the MOF of interest\n",
    "        this_neighbor_T = this_neighbor['T'] # getting the breakdown temperature value\n",
    "        neighbor_truth = str(round(this_neighbor_T.iloc[0], 1)) # extract the flag value and convert it into a string. Want just one decimal place\n",
    "\n",
    "\n",
    "    # next, getting DOI\n",
    "    if prediction_type == 'solvent':\n",
    "        this_neighbor = solvent_flags_df[solvent_flags_df['CoRE_name'] == selected_neighbor] # getting the row with the MOF of interest\n",
    "        this_neighbor_doi = this_neighbor['doi'] # getting the doi\n",
    "        this_neighbor_doi = this_neighbor_doi.iloc[0]\n",
    "    else: # thermal\n",
    "        TGA_df = pd.read_excel(MOFSIMPLIFY_PATH + 'TGA/TGA_info_log.xlsx')\n",
    "        this_neighbor = TGA_df[TGA_df['CoRE_name'] == selected_neighbor] # getting the row with the MOF of interest\n",
    "        this_neighbor_doi = this_neighbor['doi'] # getting the doi\n",
    "        this_neighbor_doi = this_neighbor_doi.iloc[0]\n",
    "\n",
    "    # Now, writing all of this information to a string\n",
    "\n",
    "    # Delete and remake the latent_neighbor folder each time, so only one file is ever in it\n",
    "    shutil.rmtree(MOFSIMPLIFY_PATH + 'temp_file_creation_' + str(session['ID']) + '/latent_neighbor')\n",
    "    os.mkdir(MOFSIMPLIFY_PATH + 'temp_file_creation_' + str(session['ID']) + '/latent_neighbor')\n",
    "\n",
    "    with open(MOFSIMPLIFY_PATH + 'temp_file_creation_' + str(session['ID']) + '/latent_neighbor/' + prediction_type + '-' + current_MOF + '-' + selected_neighbor + '.txt','w') as f:\n",
    "        f.write('Prediction type: ' + prediction_type + '\\n')\n",
    "        f.write('Current MOF: ' + current_MOF + '\\n')\n",
    "        f.write('Selected CoRE nearest neighbor in latent space: ' + selected_neighbor + '\\n')\n",
    "        f.write('Latent space distance: ' + latent_space_distance + '\\n')\n",
    "        if prediction_type == 'solvent':\n",
    "            f.write('Property for nearest neighbor: ' + neighbor_truth + '\\n')\n",
    "        else:\n",
    "            degree_sign= u'\\N{DEGREE SIGN}'\n",
    "            f.write('Property for nearest neighbor: ' + neighbor_truth + ' ' + degree_sign + 'C\\n')\n",
    "        f.write('Neighbor DOI: ' + this_neighbor_doi + '\\n')\n",
    "\n",
    "    with open(MOFSIMPLIFY_PATH + 'temp_file_creation_' + str(session['ID']) + '/latent_neighbor/' + prediction_type + '-' + current_MOF + '-' + selected_neighbor + '.txt','r') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    myDict = {'contents': contents, 'file_name': prediction_type + '-' + current_MOF + '-' + selected_neighbor + '.txt'}\n",
    "\n",
    "    return myDict\n",
    "\n",
    "@app.route('/get_descriptors', methods=['POST']) \n",
    "def descriptor_getter():\n",
    "    \"\"\"\n",
    "    descriptor_getter returns the contents of the csv with the descriptors of the desired MOF.\n",
    "    These descriptors are RACs and Zeo++ descriptors. There are 188 total.\n",
    "\n",
    "    :return: str contents, the contents of the csv with the descriptor data. To be written to a csv in the front end for download.\n",
    "    \"\"\" \n",
    "\n",
    "    # Grab data\n",
    "    name = json.loads(flask.request.get_data()); # This is the selected MOF\n",
    "    if name[-4:] == '.cif':\n",
    "        name = name[:-4] # remove the .cif part of the name\n",
    "\n",
    "    temp_file_folder = MOFSIMPLIFY_PATH + \"temp_file_creation_\" + str(session['ID']) + '/'\n",
    "    descriptors_folder = temp_file_folder + \"merged_descriptors/\"\n",
    "\n",
    "    with open(descriptors_folder + name + '_descriptors.csv', 'r') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    return contents\n",
    "\n",
    "@app.route('/get_TGA', methods=['POST']) \n",
    "def TGA_getter():\n",
    "    \"\"\"\n",
    "    TGA_getter returns the contents of the csv with the simplified TGA data of the desired MOF.\n",
    "\n",
    "    :return: str contents, the contents of the csv with the simplified TGA data. To be written to a csv in the front end for download.\n",
    "    \"\"\" \n",
    "\n",
    "    # Grab data\n",
    "    name = json.loads(flask.request.get_data()); # This is the thermal ANN latent space nearest neighbor\n",
    "    \n",
    "    # cut off these endings, in order to access the TGA file correctly:\n",
    "    # _clean, _ion_b, _neutral_b, _SL, _charged, _clean_h, _manual, _auto, _charged, etc\n",
    "    cut_index = name.find('_') # gets index of the first underscore\n",
    "    name = name[:cut_index] \n",
    "\n",
    "    tga_folder = MOFSIMPLIFY_PATH + \"TGA/raw_TGA_digitization_data/digitized_csv/\"\n",
    "\n",
    "    with open(tga_folder + name + '.csv', 'r') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    return contents\n",
    "\n",
    "@app.route('/bb', methods=['POST'])\n",
    "def stable_bb_getter():\n",
    "    \"\"\"\n",
    "    stable_bb_getter returns the contents of the XYZ of the building block specified by the front end.\n",
    "\n",
    "    :return: str contents, the contents of the XYZ file of the requested building block.\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab data\n",
    "    name = json.loads(flask.request.get_data()); # This is the building block to get.\n",
    "\n",
    "    bb_file = open(MOFSIMPLIFY_PATH + 'stable_building_blocks/building_blocks/' + name + '.xyz', 'r');\n",
    "    bb_info = bb_file.read();\n",
    "    bb_file.close();\n",
    "\n",
    "    return bb_info\n",
    "\n",
    "@app.route('/filter_MOFs', methods=['POST'])\n",
    "def filter_MOFs():\n",
    "    \"\"\"\n",
    "    filter_MOFs returns the list of stable MOFs after applying the filter criteria passed from the front end.\n",
    "\n",
    "    :return: list contents, the stable MOFs that survive all of the filters (e.g. inorganic node filter).\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab data\n",
    "    filter_dictionary = json.loads(flask.request.get_data()); # This dictionary contains the filters to apply.\n",
    "    print(f'check, filter_dictionary is {filter_dictionary}')\n",
    "\n",
    "    ultrastable_filter = filter_dictionary['ultrastable']\n",
    "    inorg_node_filter = filter_dictionary['inorg_node']\n",
    "    org_node_filter = filter_dictionary['org_node']\n",
    "    edge_filter = filter_dictionary['edge']\n",
    "    metal_filter = filter_dictionary['metal']\n",
    "    type_filter = filter_dictionary['type']\n",
    "    net_filter = filter_dictionary['net']\n",
    "\n",
    "    # Applying the type and ultrastable filter filter.\n",
    "    if ultrastable_filter == 'No':\n",
    "        if type_filter == 'Any' or len(type_filter) == 0: # # No filter applied in the 'Any' case. The empty string occurs if the user clicks on the selection box, and then clicks off without selecting anything\n",
    "            MOF_list = list_1inorganic_1edge_MOFs + list_1inorganic_1organic_1edge_MOFs + list_2inorganic_1edge_MOFs\n",
    "        elif type_filter == '1, 0, 1':\n",
    "            MOF_list = list_1inorganic_1edge_MOFs\n",
    "        elif type_filter == '1, 1, 1':\n",
    "            MOF_list = list_1inorganic_1organic_1edge_MOFs\n",
    "        elif type_filter == '2, 0, 1':\n",
    "            MOF_list = list_2inorganic_1edge_MOFs\n",
    "    elif ultrastable_filter == 'Yes':\n",
    "        if type_filter == 'Any' or len(type_filter) == 0: # # No filter applied in the 'Any' case. The empty string occurs if the user clicks on the selection box, and then clicks off without selecting anything\n",
    "            MOF_list = list_1inorganic_1edge_MOFs_ultrastable + list_1inorganic_1organic_1edge_MOFs_ultrastable + list_2inorganic_1edge_MOFs_ultrastable\n",
    "        elif type_filter == '1, 0, 1':\n",
    "            MOF_list = list_1inorganic_1edge_MOFs_ultrastable\n",
    "        elif type_filter == '1, 1, 1':\n",
    "            MOF_list = list_1inorganic_1organic_1edge_MOFs_ultrastable\n",
    "        elif type_filter == '2, 0, 1':\n",
    "            MOF_list = list_2inorganic_1edge_MOFs_ultrastable\n",
    "    else:\n",
    "        raise Exception('Unexpected value of ultrastable_filter')\n",
    "\n",
    "    filtered_MOFs = MOF_list.copy()\n",
    "\n",
    "    # Getting rid of '.cif' from each string. For nicer display on the front end (MOF name without .cif).\n",
    "    filtered_MOFs = [i.replace('.cif', '') for i in filtered_MOFs]\n",
    "\n",
    "    # Dictionary that indicates which SBUs contain which metals.\n",
    "    metal_dict = {'Gd': ['PORLAL_clean_sbu_0', 'MAKGOX_clean_sbu_0', 'INOVEN_clean_sbu_0', 'KUMBOL_clean_sbu_0', 'KUMBOL_clean_sbu_1'], 'Tb': ['MAKGUD_clean_sbu_1', 'KUMBUR_clean_sbu_0', 'KUMBUR_clean_sbu_1', 'NUHQIS_clean_sbu_0', 'NUHQUE_clean_sbu_2', 'ZALLEG_clean_sbu_0', 'NUHRAL_clean_sbu_3', 'CUKXOW_clean_sbu_0', 'XUDNAN_clean_sbu_1', 'BETDIP_clean_sbu_0', 'XADDAJ01_clean_sbu_3', 'PAMTUU_clean_sbu_0'], 'Mn': ['BELTOD_clean_sbu_0', 'KUFVIS_clean_sbu_0', 'CIFDUS_clean_sbu_1', 'ICIZOL_clean_sbu_0'], 'Dy': ['BETDEL_clean_sbu_0', 'PAMTOO_clean_sbu_0'], 'Co': ['CEKHIL_clean_sbu_0', 'JUFBIX_clean_sbu_0', 'GIZVER_clean_sbu_0', 'BEQFEK_clean_sbu_0', 'OLANAS_clean_sbu_2', 'EBIMEJ_clean_sbu_0', 'KOZSID_clean_sbu_0', 'XOMCOT_clean_sbu_1', 'OLANEW_clean_sbu_3', 'XOMCOT_clean_sbu_0', 'ICAMEG_clean_sbu_1', 'OLANEW_clean_sbu_0', 'LIZSOE_clean_sbu_0'], 'Nd': ['GEDQOX_clean_sbu_0'], 'U': ['WUSLED_clean_sbu_0'], 'Zn': ['QEWDON_clean_sbu_0', 'NAHDIM_clean_sbu_0', 'NAWXER_clean_sbu_0', 'NAHDIM_clean_sbu_2', 'FANWIC_clean_sbu_0', 'HICVOG_clean_sbu_0', 'TATPOV_clean_sbu_0', 'VOLQOD_clean_sbu_1', 'MIDCAF_clean_sbu_0', 'OLANAS_clean_sbu_1', 'FUNLAD_clean_sbu_0', 'UKALOJ_clean_sbu_0', 'MIFKUJ_clean_sbu_0'], 'Sm': ['BETFAJ_clean_sbu_0'], 'Eu': ['HISSIN_clean_sbu_1', 'HISSIN_clean_sbu_0', 'NUHQIS_clean_sbu_0', 'KUMJIN_clean_sbu_1', 'NUHQUE_clean_sbu_2', 'CUWYAW_clean_sbu_0', 'KUMJIN_clean_sbu_0', 'EYACOX_clean_sbu_1', 'EYACOX_clean_sbu_0', 'NUHRAL_clean_sbu_3', 'SARMOO_clean_sbu_0', 'IYUCEM_clean_sbu_0', 'EZIPEK_clean_sbu_0'], 'Mg': ['TAGTUT_clean_sbu_1', 'AZAVOO_clean_sbu_0', 'TAGTUT_clean_sbu_3', 'WAQDOJ_charged_sbu_0', 'LEVNOQ01_clean_sbu_1', 'EQERAU_clean_sbu_0', 'MUDLON_clean_sbu_0'], 'La': ['BETGAK_clean_sbu_0'], 'Fe': ['VETTIZ_charged_sbu_0', 'IZENUY_clean_sbu_0'], 'Na': ['OLANAS_clean_sbu_1'], 'Cd': ['AJUNOK_clean_sbu_0', 'OLANEW_clean_sbu_2', 'OLANEW_clean_sbu_3', 'QUQFOY_clean_sbu_3', 'QUQFOY_clean_sbu_1', 'OLANEW_clean_sbu_1', 'OLANEW_clean_sbu_0', 'OLANEW_clean_sbu_4'], 'Pr': ['BETFEN_clean_sbu_0'], 'Zr': ['ENOWUB_clean_sbu_0'], 'Hf': ['UKIBUN_clean_sbu_0'], 'Ho': ['BETDAH_clean_sbu_0'], 'Cu': ['BOTCEU_clean_sbu_0', 'ESEHIV_clean_sbu_0'], 'Sc': ['OJICUG_clean_sbu_0'], 'Sr': ['KAKCAD_clean_sbu_0'], 'Li': ['UXUYUI_clean_sbu_0'], 'In': ['GALJAG_clean_sbu_0']}\n",
    "\n",
    "    # Applying the rest of the filters.\n",
    "    if inorg_node_filter != 'Any' and len(inorg_node_filter) != 0: # No filter applied in the 'Any' case. The empty string occurs if the user clicks on the selection box, and then clicks off without selecting anything   \n",
    "        filtered_MOFs = [i for i in filtered_MOFs if (bb_mapping[inorg_node_filter]+'_') in i] # The underscore helps distinguish between N4 and N46, for example\n",
    "    if org_node_filter != 'Any' and len(org_node_filter) != 0:\n",
    "        filtered_MOFs = [i for i in filtered_MOFs if (bb_mapping[org_node_filter]+'_') in i] # The underscore helps distinguish between N4 and N46, for example\n",
    "    if edge_filter != 'Any' and len(edge_filter) != 0:\n",
    "        if edge_filter == 'none':\n",
    "            filtered_MOFs = [i for i in filtered_MOFs if i.endswith('none')]\n",
    "        else: # In this case, we need to de-alias using the mapping.\n",
    "            filtered_MOFs = [i for i in filtered_MOFs if i.endswith(bb_mapping[edge_filter])]\n",
    "    if metal_filter != 'Any' and len(metal_filter) != 0:\n",
    "        suitable_SBUs = metal_dict[metal_filter]\n",
    "        suitable_SBU_aliases = [bb_mapping[SBU] for SBU in suitable_SBUs]\n",
    "\n",
    "        filtered_MOFs = [i for i in filtered_MOFs if any(j in i for j in suitable_SBU_aliases)] # See if any of the SBUs with the requested metal in it are in the assessed MOF i. If so, keep.\n",
    "    if net_filter != 'Any' and len(net_filter) != 0:\n",
    "        filtered_MOFs = [i for i in filtered_MOFs if f'net-{net_filter}_node1' in i]\n",
    "\n",
    "    response_dict = {'filtered_MOFs': filtered_MOFs} # List is not a valid return type, but dictionary is.\n",
    "\n",
    "    return response_dict\n",
    "\n",
    "def type_determination(MOF_name):\n",
    "    \"\"\"\n",
    "    type_determination returns a string indicating the type of the MOF passed from the front end. Either 1,0,1; 1,1,1; or 2,0,1 for #inorganic nodes, #organic nodes, #edges \n",
    "\n",
    "    :param MOF_name: str, the name of the MOF for which the type will be ascertained.\n",
    "    :return: str MOF_type, the type of the MOF in question.\n",
    "    \"\"\"\n",
    "    if MOF_name in list_1inorganic_1edge_MOFs:\n",
    "        MOF_type = '1inorganic_1edge'\n",
    "    elif MOF_name in list_1inorganic_1organic_1edge_MOFs:\n",
    "        MOF_type = '1inorganic_1organic_1edge'\n",
    "    elif MOF_name in list_2inorganic_1edge_MOFs:\n",
    "        MOF_type = '2inorganic_1edge'\n",
    "    else:\n",
    "        raise Exception('CIF name is unexpected. Not in any of the MOF lists.')\n",
    "\n",
    "    return MOF_type\n",
    "\n",
    "@app.route('/data_pull', methods=['POST'])\n",
    "def grab_data():\n",
    "    \"\"\"\n",
    "    grab_data returns a dictionary of information on the MOF whose name is sent from the front end.\n",
    "\n",
    "    :return: dict data_dict, a dictionary with stability and uptake information on the selected MOF.\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab data\n",
    "    name = json.loads(flask.request.get_data()); # This is the MOF about which to gather data.    \n",
    "    MOF_type = type_determination(name + '.cif')\n",
    "    if MOF_type == '1inorganic_1edge':\n",
    "        type_abbrev = '1inor_1edge'\n",
    "    elif MOF_type == '1inorganic_1organic_1edge':\n",
    "        type_abbrev = '1inor_1org_1edge'\n",
    "    elif MOF_type == '2inorganic_1edge':\n",
    "        type_abbrev = '2inor_1edge'\n",
    "\n",
    "    # First, getting ANN solvent removal and thermal stabilities\n",
    "    df = pd.read_csv(f'stable_building_blocks/predictions/{type_abbrev}_predictions.csv')\n",
    "    adjusted_name = name.replace('optimized_', '') + '.cif' # To match the format of entries in the predictions excel files.\n",
    "\n",
    "    try:\n",
    "        desired_row = df[df['filename'] == adjusted_name]\n",
    "        predicted_solvent_removal_stability = desired_row['predicted_solvent_removal_stability'].iloc[0]\n",
    "        predicted_thermal_stability = desired_row['predicted_thermal_stability'].iloc[0]\n",
    "\n",
    "        # Reducing decimal places. 2 for solvent removal stability, 1 for thermal stability.\n",
    "        predicted_solvent_removal_stability = predicted_solvent_removal_stability.round(2)\n",
    "        predicted_thermal_stability = predicted_thermal_stability.round(1)\n",
    "\n",
    "    except IndexError: # adjusted_name is not present in the CSV\n",
    "        predicted_solvent_removal_stability = None\n",
    "        predicted_thermal_stability = None\n",
    "\n",
    "    # Next, getting mechanical moduli\n",
    "    df = pd.read_csv(f'stable_building_blocks/computed_properties/stable/moduli_{type_abbrev}.csv')\n",
    "    try:\n",
    "        desired_row = df[df['name'] == 'optimized_'+name] # Don't need to use adjusted_name, since the moduli CSV files have a slightly different format for the name column.\n",
    "        moduli_info = {} # Dictionary to be populated.\n",
    "        moduli_properties = ['KR', 'KV', 'KVRH', 'GR', 'GV', 'GVRH']\n",
    "        for prop in moduli_properties:\n",
    "            moduli_info[prop] = desired_row[prop].iloc[0].round(1) # Round to one decimal place.\n",
    "    except IndexError: # adjusted_name is not present in the CSV\n",
    "        moduli_info = None\n",
    "\n",
    "    # Next, getting uptake information\n",
    "    df = pd.read_csv(f'stable_building_blocks/computed_properties/stable/uptake_{type_abbrev}.csv')\n",
    "    try:\n",
    "        desired_row = df[df['filename'] == adjusted_name]\n",
    "        uptake_info = {} # Dictionary to be populated.\n",
    "        uptake_properties = ['uptake_low_p (cm3 methane/g framework)', 'uptake_low_p (cm3 methane/cm3 framework)', 'uptake_high_p (cm3 methane/g framework)', 'uptake_high_p (cm3 methane/cm3 framework)']\n",
    "        for prop in uptake_properties:\n",
    "            uptake_info[prop] = desired_row[prop].iloc[0].round(2) # Round to two decimal places.\n",
    "    except IndexError: # adjusted_name is not present in the CSV\n",
    "        uptake_info = None\n",
    "\n",
    "    # Lastly, assessing whether MOF is ultrastable\n",
    "    is_ultrastable = adjusted_name in ultrastable_MOFs\n",
    "\n",
    "    data_dict = {\n",
    "    'solvent_removal_stability': predicted_solvent_removal_stability, \n",
    "    'thermal_stability': predicted_thermal_stability,\n",
    "    'moduli_info': moduli_info,\n",
    "    'uptake_info': uptake_info,\n",
    "    'is_ultrastable': is_ultrastable\n",
    "    }\n",
    "    \n",
    "    print(data_dict)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "@app.route('/opt_geo_existence_check', methods=['POST'])\n",
    "def existence_check():\n",
    "    \"\"\"\n",
    "    existence_check returns a string indicating whether the optimized geometry for the requested MOF exists.\n",
    "\n",
    "    :return: str exists, a string indicating whether the optimized CIF file is available for download.\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab data\n",
    "    name = json.loads(flask.request.get_data()); # This is the MOF about which to gather data.    \n",
    "    MOF_type = type_determination(name + '.cif')\n",
    "\n",
    "    exists = os.path.isfile(f'stable_building_blocks/optimized_structures/{MOF_type}/optimized_{name}.cif')\n",
    "    if exists:\n",
    "        exists = 'Yes'\n",
    "    else:\n",
    "        exists = 'No'\n",
    "\n",
    "    return exists\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8000, debug=True)\n",
    "    #app.run(host='0.0.0.0', port=8000, threaded=False, processes=10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
